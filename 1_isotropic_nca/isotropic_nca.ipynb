{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility Functions\n",
    "- NOTE: tensors are organized as follows: [BATCH_SIZE, CHANNELS, WIDTH, HEIGHT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce GTX 1660 Ti (UUID: GPU-d77bcb98-b41a-4494-1c93-f4f7d0b55d9c)\n",
      "cuda available?  True\n",
      "device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as func\n",
    "import PIL.Image\n",
    "import random\n",
    "import json\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import matplotlib.pylab as pl\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from torchvision.transforms.functional_tensor import gaussian_blur\n",
    "import torchvision.transforms.functional as trans\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "\n",
    "# * loads an image and converts to a tensor\n",
    "# *     default tensor shape: [BATCH_SIZE (1), CHANNELS (4), WIDTH (_size), HEIGHT (_size)]\n",
    "def load_image_as_tensor(_path, _size, _resample=PIL.Image.Resampling.BICUBIC):\n",
    "    img = PIL.Image.open(_path)\n",
    "    img = img.resize((_size, _size), _resample)\n",
    "    img = np.float32(img) / 255.0\n",
    "    img[..., :3] *= img[..., 3:]\n",
    "    return torch.from_numpy(img).permute(2, 0, 1)[None, ...]\n",
    "\n",
    "# * given a tensor of default shape, visualize the first 4 channels as a RGBA image\n",
    "def show_tensor_as_image(_tensor):\n",
    "    img = to_rgb(_tensor).squeeze().permute(1, 2, 0)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "# * takes the first 4 channels of a tensor of default shape and converts to a RGB image\n",
    "def to_rgb(_x, _alpha='BLACK'):\n",
    "    rgb, a = _x[:, :3], _x[:, 3:4]\n",
    "    if _alpha == 'BLACK':\n",
    "        return torch.clamp(rgb, 0.0, 1.0)\n",
    "    elif _alpha == 'WHITE':\n",
    "        return torch.clamp(1.-a + rgb, 0.0, 1.0)\n",
    "\n",
    "# * creates a circle mask centered at a position of a given radius\n",
    "def circle_mask(_size, _radius, _pos):\n",
    "    Y, X = np.ogrid[:_size, :_size]\n",
    "    dist_from_center = np.sqrt((X - _pos[0])**2 + (Y-_pos[1])**2)\n",
    "    mask = dist_from_center >= _radius\n",
    "    return mask\n",
    "\n",
    "# * shows a batch before and after a forward pass given two (2) tensors\n",
    "def show_batch(_batch_size, _before, _after, _dpi=256):\n",
    "    fig = plt.figure(figsize=(_batch_size, 2), dpi=_dpi)\n",
    "    axarr = fig.subplots(nrows=2, ncols=_batch_size)\n",
    "    gspec = gridspec.GridSpec(2, _batch_size)\n",
    "    gspec.update(wspace=0.1, hspace=0) # set the spacing between axes.\n",
    "    plt.clf()\n",
    "    for i in range(_batch_size):\n",
    "        img_i = _before[i].unsqueeze(0)\n",
    "        img_rgb = to_rgb(img_i, _alpha='BLACK').squeeze().permute(1, 2, 0)\n",
    "        axarr[0, i] = plt.subplot(gspec[i])\n",
    "        axarr[0, i].set_xticks([])\n",
    "        axarr[0, i].set_yticks([])\n",
    "        axarr[0, i].imshow(img_rgb, aspect='equal')\n",
    "        axarr[0, i].set_title(str(i), fontsize=8)   \n",
    "    for i in range(_batch_size):\n",
    "        img_i = _after[i].unsqueeze(0)\n",
    "        img_rgb = to_rgb(img_i, _alpha='BLACK').squeeze().permute(1, 2, 0)\n",
    "        axarr[1, i] = plt.subplot(gspec[i+_batch_size])\n",
    "        axarr[1, i].set_xticks([])\n",
    "        axarr[1, i].set_yticks([])\n",
    "        axarr[1, i].imshow(img_rgb, aspect='equal') \n",
    "    plt.show()\n",
    "\n",
    "# * blurs an image using gaussian blur\n",
    "def unsharpen(_img):\n",
    "    blur = gaussian_blur(_img, (5, 5), (1, 1))\n",
    "    return _img + (_img - blur) * 2.0\n",
    "\n",
    "# * find GPU available\n",
    "clear_output()\n",
    "!nvidia-smi -L\n",
    "\n",
    "# * sets the device\n",
    "# *     defaults to 'cuda'\n",
    "_DEVICE_ = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print ('cuda available? ', torch.cuda.is_available())\n",
    "print ('device: ', _DEVICE_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Pre-Made Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed_img.shape:  torch.Size([1, 4, 64, 64])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAFJ0lEQVR4nO3bMYrDQBQFQc2i+195nCwdrUAGSxqzVaFw8LLmM3jMOecGANu2/Tw9AIB1iAIAEQUAIgoARBQAiCgAEFEAIKIAQPazPxxjXLkDgIud+a+ySwGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAg+9MD4C3z4Pu4dcWvv8Y8MgQ+xqUAQEQBgIgCABEFAOKhme+y1DvuUmPgI1wKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCANmfHgB3mwffx60rYE0uBQAiCgBEFACIKAAQD838Ox6U4ZhLAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACD72R/OOa/cAcACXAoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAOQF8JkQEioui+4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAFEElEQVR4nO3ZMQrDMBBFQSn4/lfeFIFXOUUg2IHMlKp+91i0Z2YWAKy1HncPAOB3iAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQI67B8An9tqn77Pm4iVr7ZMpc/0M+CqXAgARBQAiCgBEFADInvE1BsCLSwGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgx90D4HJ7n7/PXLsDfpBLAYCIAgARBQAiCgDERzP/x4cyvOVSACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgT+NrEwVG5ITHAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_SEED_FILE_ = '_3_seeds_64.png'\n",
    "_SIZE_ = 64\n",
    "_SEED_ANGLE_RAD_ = (1)*np.pi\n",
    "\n",
    "seed_img = load_image_as_tensor('..\\\\_seeds\\\\'+_SEED_FILE_,  _SIZE_)\n",
    "seed_img = trans.rotate(seed_img, np.rad2deg(_SEED_ANGLE_RAD_))\n",
    "print ('seed_img.shape: ', seed_img.shape)\n",
    "show_tensor_as_image(to_rgb(seed_img))\n",
    "show_tensor_as_image(to_rgb(seed_img, 'WHITE'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target Image to Train Model\n",
    "- NOTE: seed _SIZE_ should equal _TARGET-SIZE_ + (2 * _PAD_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_img.shape:  torch.Size([1, 4, 64, 64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_TARGET_FILE_ = 'bonzai.png'\n",
    "_TARGET_SIZE_ = 40\n",
    "_PAD_ = 12\n",
    "\n",
    "target_img = load_image_as_tensor('..\\\\_images\\\\'+_TARGET_FILE_, _TARGET_SIZE_)\n",
    "target_img = torch.nn.functional.pad(target_img, (_PAD_, _PAD_, _PAD_, _PAD_), 'constant', 0)\n",
    "print ('target_img.shape: ', target_img.shape)\n",
    "show_tensor_as_image(target_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptions:\n",
    "- LAPLACIAN: isotropic nca model\n",
    "- SOBEL_MAG: isotrpic nca variant which adds upon the 'laplacian' model by making use of the magnitude of the two directional sobel filters\n",
    "- STEERABLE: angle-based steerable nca\n",
    "- GRADIENT: gradient-based steerable nca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_SOBEL_DIV_ = 8.0\n",
    "_LAP_DIV_ = 12.0\n",
    "\n",
    "SOBEL_KERN = torch.tensor([\n",
    "    [-1., 0., 1.], \n",
    "    [-2., 1., 2.], \n",
    "    [-1., 0., 1.]])\n",
    "LAP_KERN = torch.tensor([\n",
    "    [1.,   2., 1.], \n",
    "    [2., -12., 2.], \n",
    "    [1.,   2., 1.]])\n",
    "ID_KERN = torch.tensor([\n",
    "    [0., 0., 0.], \n",
    "    [0., 1., 0.], \n",
    "    [0., 0., 0.]])\n",
    "\n",
    "# * performs a convolution per filter per channel\n",
    "def per_channel_conv(_x, _filters):\n",
    "    batch_size, channels, height, width = _x.shape\n",
    "    # * reshape x to make per-channel convolution possible + pad 1 on each side\n",
    "    y = _x.reshape(batch_size*channels, 1, height, width)\n",
    "    y = func.pad(y, (1, 1, 1, 1), 'circular')\n",
    "    # send to current device\n",
    "    _filters = _filters.to(_DEVICE_)\n",
    "    y = y.to(_DEVICE_)\n",
    "    # * perform per-channel convolutions\n",
    "    y = func.conv2d(y, _filters[:, None])\n",
    "    y = y.reshape(batch_size, -1, height, width)\n",
    "    return y\n",
    "\n",
    "# * only uses laplacian operator for local perception\n",
    "def laplacian_perception(_x):\n",
    "    # * add an extra dimention to account for batch size\n",
    "    lap_conv = per_channel_conv(_x, LAP_KERN[None, :]/_LAP_DIV_)\n",
    "    # * concat perception w/ self (identity)\n",
    "    y = torch.cat([_x, lap_conv], 1)\n",
    "    return y\n",
    "\n",
    "# * uses laplacian operator and sobel-magnitude (G) for local perception\n",
    "def sobel_mag_perception(_x):\n",
    "    # * add an extra dimention to account for batch size\n",
    "    lap_conv = per_channel_conv(_x, LAP_KERN[None, :]/_LAP_DIV_)\n",
    "    # * compute sobel-magnitude (G)\n",
    "    sobel_conv = per_channel_conv(_x, torch.stack([SOBEL_KERN/_SOBEL_DIV_, SOBEL_KERN.T/_SOBEL_DIV_]))\n",
    "    gx, gy = sobel_conv[:, ::2], sobel_conv[:, 1::2]\n",
    "    # * concat perceptions w/ self (identity)\n",
    "    y = torch.cat([_x, lap_conv, (gx*gx+gy*gy+1e-8).sqrt()], 1)\n",
    "    return y\n",
    "\n",
    "\"\"\"\n",
    "def steerable_perception(_x):\n",
    "    # * separate states and angle channels\n",
    "    states, angle = _x[:, :-1], _x[:, -1:]\n",
    "    filters = torch.stack([SOBEL_KERN, SOBEL_KERN.T])\n",
    "    grad = per_channel_conv(states, filters)\n",
    "    gx, gy = grad[:, ::2], grad[:, 1::2]\n",
    "    # * get cos and sin of angle channel and apply rotation to gx, gy\n",
    "    c, s = angle.cos(), angle.sin()\n",
    "    rot_grad = torch.cat([gx*c+gy*s, gy*c-gx*s], 1)\n",
    "    lap_conv = per_channel_conv(_x, LAP_KERN[None, :])\n",
    "    y = torch.cat([states, rot_grad, lap_conv], 1)\n",
    "\n",
    "def gradient_perception(_x):\n",
    "    filters = torch.stack([SOBEL_KERN, SOBEL_KERN.T])\n",
    "    grad = per_channel_conv(_x, filters)\n",
    "    grad, dir = grad[:, :-2], grad[:, -2:]\n",
    "    dir = dir / dir.norm(dim=1, keepdim=True).clip(1.0)\n",
    "    gx, gy = grad[:, ::2], grad[:, 1::2]\n",
    "    c, s = dir[:, :1], dir[:, 1::2]\n",
    "    rot_grad = torch.cat([gx*c+gy*s, gy*c-gx*s], 1)\n",
    "    lap_conv = per_channel_conv(_x, LAP_KERN[None, :])\n",
    "    y = torch.cat([_x, rot_grad, lap_conv], 1)\n",
    "    return y\n",
    "\"\"\"\n",
    "    \n",
    "perception = {\n",
    "    'LAPLACIAN': laplacian_perception,\n",
    "    'SOBEL_MAG': sobel_mag_perception,\n",
    "    #'STEERABLE': steerable_perception,\n",
    "    #'GRADIENT': gradient_perception    \n",
    "}\n",
    "\n",
    "def get_alive_mask(_x):\n",
    "    return func.max_pool2d(_x[:, 3:4, :, :], kernel_size=3, stride=1, padding=1) > 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a.shape:  torch.Size([1, 3, 3, 3])\n",
      "type(a):  <class 'torch.Tensor'>\n",
      "a.dtype:  torch.float32\n",
      "a:  tensor([[[[ 1.3586e+00, -4.8168e-01,  3.0446e-01],\n",
      "          [-1.3346e+00,  3.8071e-01,  1.6793e+00],\n",
      "          [-2.2916e-01, -1.0790e+00, -3.5778e-01]],\n",
      "\n",
      "         [[-8.4892e-01,  9.3372e-01,  5.4554e-01],\n",
      "          [-1.1410e+00, -8.2813e-05,  3.7987e-01],\n",
      "          [ 1.6393e+00, -1.9365e+00, -1.5368e-01]],\n",
      "\n",
      "         [[-6.0248e-01,  1.5953e-01,  3.2442e-02],\n",
      "          [ 1.0681e+00, -1.1016e+00,  1.1938e+00],\n",
      "          [-2.2666e-01,  1.1088e+00,  9.3820e-01]]]], device='cuda:0')\n",
      "b.shape:  torch.Size([1, 6, 3, 3])\n",
      "type(b):  <class 'torch.Tensor'>\n",
      "b.dtype:  torch.float32\n",
      "b:  tensor([[[[ 1.3586e+00, -4.8168e-01,  3.0446e-01],\n",
      "          [-1.3346e+00,  3.8071e-01,  1.6793e+00],\n",
      "          [-2.2916e-01, -1.0790e+00, -3.5778e-01]],\n",
      "\n",
      "         [[-8.4892e-01,  9.3372e-01,  5.4554e-01],\n",
      "          [-1.1410e+00, -8.2813e-05,  3.7987e-01],\n",
      "          [ 1.6393e+00, -1.9365e+00, -1.5368e-01]],\n",
      "\n",
      "         [[-6.0248e-01,  1.5953e-01,  3.2442e-02],\n",
      "          [ 1.0681e+00, -1.1016e+00,  1.1938e+00],\n",
      "          [-2.2666e-01,  1.1088e+00,  9.3820e-01]],\n",
      "\n",
      "         [[-1.5969e+00,  6.2228e-01, -1.2656e-01],\n",
      "          [ 1.7317e+00, -4.9371e-01, -1.8831e+00],\n",
      "          [ 1.5059e-01,  1.1317e+00,  4.6396e-01]],\n",
      "\n",
      "         [[ 1.0360e+00, -1.2467e+00, -6.1357e-01],\n",
      "          [ 1.2851e+00, -1.9539e-01, -5.2245e-01],\n",
      "          [-2.1644e+00,  2.2510e+00,  1.7035e-01]],\n",
      "\n",
      "         [[ 9.5299e-01, -5.5410e-03,  3.1979e-01],\n",
      "          [-1.0044e+00,  1.7018e+00, -1.0010e+00],\n",
      "          [ 6.6912e-01, -1.0062e+00, -6.2649e-01]]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# * experiments and tests\n",
    "\n",
    "a = torch.randn([1, 3, 3, 3]).to(_DEVICE_)\n",
    "print ('a.shape: ', a.shape)\n",
    "print ('type(a): ', type(a))\n",
    "print ('a.dtype: ', a.dtype)\n",
    "print ('a: ', a)\n",
    "\n",
    "b = laplacian_perception(a)\n",
    "print ('b.shape: ', b.shape)\n",
    "print ('type(b): ', type(b))\n",
    "print ('b.dtype: ', b.dtype)\n",
    "print ('b: ', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isotropic Neural Cellular Automata Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perception channels:  48\n",
      "hidden channels:  128\n",
      "ISO-NCA param count: 8320\n"
     ]
    }
   ],
   "source": [
    "_CHANNELS_ = 16\n",
    "_HIDDEN_ = 128\n",
    "_MODEL_TYPE_ = 'SOBEL_MAG'  #['LAPLACIAN', 'SOBEL_MAG', 'STEERABLE', 'GRADIENT']\n",
    "_STOCHASTIC_UPDATE_RATE_ = 0.5\n",
    "# * last state channel is angle and should be treated differently\n",
    "_ANGLE_CHANNEL_ = 1 if _MODEL_TYPE_ == 'STEERABLE' else 0\n",
    "_SCALAR_CHANNEL_ = _CHANNELS_ - _ANGLE_CHANNEL_\n",
    "\n",
    "class ISO_NCA(torch.nn.Module):\n",
    "    def __init__(self, _channels=_CHANNELS_, _hidden=_HIDDEN_, _device=_DEVICE_):\n",
    "        super().__init__()\n",
    "        self.device = _device\n",
    "        # * determine number of perceived channels\n",
    "        perception_channels = perception[_MODEL_TYPE_](torch.zeros([1, _channels, 8, 8]).to(_device)).shape[1]\n",
    "        \n",
    "        # * determine hidden channels (equalize the parameter count btwn model types)\n",
    "        hidden_channels = 8*1024 // (perception_channels+_channels)\n",
    "        hidden_channels = (_hidden+31) // 32*32\n",
    "        \n",
    "        print ('perception channels: ', perception_channels)\n",
    "        print ('hidden channels: ', hidden_channels)\n",
    "        \n",
    "        # * model layers\n",
    "        self.conv1 = torch.nn.Conv2d(perception_channels, hidden_channels, 1)\n",
    "        self.conv2 = torch.nn.Conv2d(hidden_channels, _channels, 1, bias=False)\n",
    "        with torch.no_grad():\n",
    "            self.conv2.weight.data.zero_()\n",
    "        \n",
    "        # * send to device\n",
    "        self.to(_device)\n",
    "        \n",
    "    def forward(self, _x):\n",
    "        # * get alive mask\n",
    "        alive_mask = get_alive_mask(_x).to(self.device)\n",
    "        \n",
    "        # * perception step\n",
    "        _x = _x.to(self.device)\n",
    "        p = perception[_MODEL_TYPE_](_x)\n",
    "        \n",
    "        # * update step\n",
    "        p = self.conv2(torch.relu(self.conv1(p)))\n",
    "        \n",
    "        # * create stochastic update mask\n",
    "        stochastic_mask = (torch.rand(_x[:, :1, :, :].shape) <= _STOCHASTIC_UPDATE_RATE_).to(self.device, torch.float32)\n",
    "        \n",
    "        # * perform update\n",
    "        _x = _x + p * stochastic_mask\n",
    "        if _SCALAR_CHANNEL_ == _CHANNELS_:\n",
    "            _x = _x * alive_mask\n",
    "        else:\n",
    "            _x = torch.cat([_x[:, :_SCALAR_CHANNEL_]*alive_mask, _x[:, _SCALAR_CHANNEL_:] % (np.pi*2.0)], 1)\n",
    "        return _x\n",
    "\n",
    "# * print model parameter count\n",
    "param_n = sum(p.numel() for p in ISO_NCA().parameters())\n",
    "print('ISO-NCA param count:', param_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Functions:\n",
    "- PIXEL-WISE: L2-norm loss\n",
    "- INVARIANT: rotation-invariant loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "_LOSS_FUNC_ = 'PIXEL_WISE' #['PIXEL_WISE', 'INVARIANT']\n",
    "\n",
    "# r = torch.linspace(0.5/_SIZE_, 1, _SIZE_//2.0)[:, None]\n",
    "# a = torch.range(0, _SIZE_*np.pi)/(_SIZE_/2)\n",
    "# polar_xy = torch.stack([r*a.cos(), r*a.sin()], -1)[None, :]\n",
    "# polar_target = func.grid_sample(unsharpen(target_img[None, ...]), polar_xy)\n",
    "\n",
    "# x = torch.linspace(-1, 1, _SIZE_)\n",
    "# y, x = torch.meshgrid(x, x)\n",
    "# xy_grid = torch.stack([x, y], -1)\n",
    "# fft_target = torch.fft.rfft(polar_target).conj()\n",
    "# polar_target_sqnorm = polar_target.square().sum(-1, keepdim=True)\n",
    "\n",
    "def pixel_wise_loss_func(_x, _target, _scale=1e3, _dims=[]):\n",
    "    return _scale * torch.mean(torch.square(_x[:, :4] - _target), _dims)\n",
    "\n",
    "# def invariant_losses_func(_x):\n",
    "#     img = unsharpen(_x)\n",
    "#     polar_img = func.grid_sample(img, polar_xy.repeat(len(img), 1, 1, 1), mode='bicubic')\n",
    "#     x = torch.fft.rfft(polar_img)\n",
    "#     xy = torch.fft.irfft(x*fft_target)\n",
    "#     xx = polar_img.square().sum(-1, keepdim=True)\n",
    "#     yy = polar_target_sqnorm\n",
    "#     diff = xx+yy-2.0*xy\n",
    "#     return diff.mean([1, 2])\n",
    "\n",
    "def invariant_loss_func(_x):\n",
    "    raise NotImplementedError\n",
    "    # return invariant_losses_func(_x).min(-1)[0].mean()\n",
    "\n",
    "loss_func = {\n",
    "    'PIXEL_WISE': pixel_wise_loss_func,\n",
    "    'INVARIANT': invariant_loss_func\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create ISO-NCA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perception channels:  48\n",
      "hidden channels:  128\n"
     ]
    }
   ],
   "source": [
    "_NAME_ = 'sobel_mag_bonzai_3_seeds_v1'\n",
    "_POOL_SIZE_ = 256\n",
    "_BATCH_SIZE_ = 8\n",
    "_LOWER_LR_ = 1e-5\n",
    "_UPPER_LR_ = 1e-3\n",
    "\n",
    "# * create model / optimizer / lr-scheduler\n",
    "model = ISO_NCA()\n",
    "opt = torch.optim.Adam(model.parameters(), _UPPER_LR_)\n",
    "lr_sched = torch.optim.lr_scheduler.CyclicLR(opt, _LOWER_LR_, _UPPER_LR_, step_size_up=2000, mode='triangular2', cycle_momentum=False)\n",
    "\n",
    "# * create target batch\n",
    "target_batch = target_img.clone().repeat(_BATCH_SIZE_, 1, 1, 1).to(_DEVICE_)\n",
    "\n",
    "# * create seed pool\n",
    "seed = torch.cat([seed_img, torch.zeros([1, _CHANNELS_-4, _SIZE_, _SIZE_])], 1).to(_DEVICE_)\n",
    "with torch.no_grad():\n",
    "    pool = seed.clone().repeat(_POOL_SIZE_, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load ISO-NCA Model Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = False\n",
    "load_from = 'checkpoints\\\\sobel_mag_cowboy_v2_cp14000'\n",
    "\n",
    "def load_model_checkpoint():\n",
    "    if not load_model: \n",
    "        return\n",
    "\n",
    "    # * open params json file\n",
    "    params = {}\n",
    "    with open(load_from + '_params.json', 'r') as openfile:\n",
    "        params = json.load(openfile)\n",
    "    \n",
    "    # * load in params\n",
    "    _DEVICE_ = params['_DEVICE_']\n",
    "    _SEED_FILE_ = params['_SEED_FILE_']\n",
    "    _SIZE_ = params['_SIZE_']\n",
    "    _SEED_ANGLE_RAD_ = params['_SEED_ANGLE_RAD_']\n",
    "    _NAME_ = params['_NAME_']\n",
    "    _MODEL_TYPE_ = params['_MODEL_TYPE_']\n",
    "    _POOL_SIZE_ = params['_POOL_SIZE_']\n",
    "    _TARGET_FILE_ = params['_TARGET_FILE_']\n",
    "    _TARGET_SIZE_ = params['_TARGET_SIZE_']\n",
    "    _PAD_ = params['_PAD_']\n",
    "    _SOBEL_DIV_ = params['_SOBEL_DIV_']\n",
    "    _LAP_DIV_ = params['_LAP_DIV_']\n",
    "    _BATCH_SIZE_ = params['_BATCH_SIZE_']\n",
    "    _LOWER_LR_ = params['_LOWER_LR_']\n",
    "    _UPPER_LR_ = params['_UPPER_LR_']\n",
    "    _LOSS_FUNC_ = params['_LOSS_FUNC_']\n",
    "    \n",
    "    # * load state dictionary\n",
    "    model.load_state_dict(torch.load(load_from + '.pt', map_location=_DEVICE_))   \n",
    "    model.train()\n",
    "\n",
    "    # * load seed\n",
    "    seed_img = load_image_as_tensor('..\\\\_seeds\\\\'+_SEED_FILE_, _SIZE_)\n",
    "    seed_img = trans.rotate(seed_img, np.rad2deg(_SEED_ANGLE_RAD_))\n",
    "    seed = torch.cat([seed_img, torch.zeros([1, _CHANNELS_-4, _SIZE_, _SIZE_])], 1).to(_DEVICE_)\n",
    "    with torch.no_grad():\n",
    "        pool = seed.clone().repeat(_POOL_SIZE_, 1, 1, 1)\n",
    "        \n",
    "    # * load target\n",
    "    target_img = load_image_as_tensor('..\\\\_images\\\\'+_TARGET_FILE_, _TARGET_SIZE_)\n",
    "    target_img = torch.nn.functional.pad(target_img, (_PAD_, _PAD_, _PAD_, _PAD_), 'constant', 0)\n",
    "    target_batch = target_img.clone().repeat(_BATCH_SIZE_, 1, 1, 1).to(_DEVICE_)\n",
    "    \n",
    "    # * setup loss function etc.\n",
    "    opt = torch.optim.Adam(model.parameters(), _UPPER_LR_)\n",
    "    lr_sched = torch.optim.lr_scheduler.CyclicLR(opt, _LOWER_LR_, _UPPER_LR_, step_size_up=2000, mode='triangular2', cycle_momentum=False)\n",
    "    \n",
    "    print ('model loaded in successfully')\n",
    "\n",
    "load_model_checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_TRAIN_MODEL_ = True\n",
    "_EPOCHS_ = 20_000\n",
    "_NUM_DAMG_ = 4\n",
    "_DAMG_RATE_ = 2\n",
    "_INFO_RATE_ = 20\n",
    "_SAVE_RATE_ = 1000\n",
    "\n",
    "# * save model method\n",
    "def save_model(_dir, _model, _name):\n",
    "    model_path = pathlib.Path(_dir)\n",
    "    model_path.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(_model.state_dict(), _dir + '\\\\' + _name + '.pt')\n",
    "    \n",
    "    # * save model parameters\n",
    "    dict = {\n",
    "        # seed info\n",
    "        '_SEED_FILE_': _SEED_FILE_,\n",
    "        '_SIZE_': _SIZE_,\n",
    "        '_SEED_ANGLE_RAD_':_SEED_ANGLE_RAD_,\n",
    "        # target image info\n",
    "        '_TARGET_FILE_': _TARGET_FILE_,\n",
    "        '_TARGET_SIZE_': _TARGET_SIZE_,\n",
    "        '_PAD_': _PAD_,\n",
    "        # kernel dividers\n",
    "        '_SOBEL_DIV_': _SOBEL_DIV_,\n",
    "        '_LAP_DIV_': _LAP_DIV_,\n",
    "        # model info\n",
    "        '_CHANNELS_': _CHANNELS_,\n",
    "        '_HIDDEN_': _HIDDEN_,\n",
    "        '_MODEL_TYPE_': _MODEL_TYPE_,\n",
    "        '_STOCHASTIC_UPDATE_RATE_': _STOCHASTIC_UPDATE_RATE_,\n",
    "        '_ANGLE_CHANNEL_': _ANGLE_CHANNEL_,\n",
    "        '_SCALAR_CHANNEL_': _SCALAR_CHANNEL_,\n",
    "        # loss/lr info\n",
    "        '_LOSS_FUNC_': _LOSS_FUNC_,\n",
    "        '_LOWER_LR_': _LOWER_LR_,\n",
    "        '_UPPER_LR_': _UPPER_LR_,\n",
    "        # training info\n",
    "        '_DEVICE_': _DEVICE_,\n",
    "        '_NAME_': _NAME_,\n",
    "        '_EPOCHS_': _EPOCHS_,\n",
    "        '_POOL_SIZE_': _POOL_SIZE_,\n",
    "        '_BATCH_SIZE_': _BATCH_SIZE_,\n",
    "        '_NUM_DAMG_': _NUM_DAMG_,\n",
    "        # training rate info\n",
    "        '_DAMG_RATE_': _DAMG_RATE_,\n",
    "        '_INFO_RATE_': _INFO_RATE_,\n",
    "        '_SAVE_RATE_': _SAVE_RATE_,\n",
    "    }\n",
    "    json_object = json.dumps(dict, indent=4)\n",
    "    with open(_dir + '\\\\' + _name + '_params.json', 'w') as outfile:\n",
    "        outfile.write(json_object)\n",
    "    print ('model + params saved!')\n",
    "\n",
    "loss_log = []\n",
    "progress = 0\n",
    "\n",
    "# * begin training \n",
    "for _ in tqdm(range(_EPOCHS_)):\n",
    "    if not _TRAIN_MODEL_:\n",
    "        print ('skipping training')\n",
    "        break\n",
    "    with torch.no_grad():\n",
    "        # * sample batch from pool\n",
    "        i = len(loss_log)\n",
    "        batch_idxs = np.random.choice(_POOL_SIZE_, _BATCH_SIZE_, replace=False)\n",
    "        x = pool[batch_idxs]\n",
    "        \n",
    "        # * re-order batch based on loss\n",
    "        loss_ranks = torch.argsort(loss_func[_LOSS_FUNC_](x, target_batch, _dims=[-2, -3, -1]), descending=True)\n",
    "        x = x[loss_ranks]\n",
    "        \n",
    "        # * re-add seed into batch\n",
    "        x[:1] = seed\n",
    "            \n",
    "        # * damage lowest loss in batch\n",
    "        if i % _DAMG_RATE_ == 0:\n",
    "            radius = random.uniform(_SIZE_*0.05, _SIZE_*0.2)\n",
    "            u = random.uniform(0, 1) * _SIZE_\n",
    "            v = random.uniform(0, 1) * _SIZE_\n",
    "            mask = circle_mask(_SIZE_, radius, [u, v])\n",
    "            x[-_NUM_DAMG_:] *= torch.tensor(mask).to(_DEVICE_)\n",
    "            \n",
    "    # * different loss values\n",
    "    overflow_loss = 0.0\n",
    "    diff_loss = 0.0\n",
    "    target_loss = 0.0\n",
    "    \n",
    "    # * save batch before\n",
    "    if i % _INFO_RATE_ == 0:\n",
    "        before = x.detach().cpu()\n",
    "    \n",
    "    # * forward pass\n",
    "    num_steps = np.random.randint(64, 96)\n",
    "    for _ in range(num_steps):\n",
    "        prev_x = x\n",
    "        x = model(x)\n",
    "        diff_loss += (x - prev_x).abs().mean()\n",
    "        overflow_loss += (x - x.clamp(-2.0, 2.0))[:, :_SCALAR_CHANNEL_].square().sum()\n",
    "    \n",
    "    # * calculate losses\n",
    "    target_loss += loss_func[_LOSS_FUNC_](x, target_batch)\n",
    "    target_loss /= 2.0\n",
    "    diff_loss *= 10.0\n",
    "    loss = target_loss + overflow_loss + diff_loss\n",
    "    \n",
    "    # * backward pass\n",
    "    with torch.no_grad():\n",
    "        loss.backward()\n",
    "        # * normalize gradients \n",
    "        for p in model.parameters():\n",
    "            p.grad /= (p.grad.norm()+1e-8) \n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        lr_sched.step()\n",
    "        # * re-add batch to pool\n",
    "        pool[batch_idxs] = x\n",
    "        loss_log.append(loss.item())\n",
    "        \n",
    "        # * print out info\n",
    "        if i % _INFO_RATE_ == 0:\n",
    "            # * show loss plot\n",
    "            clear_output(True)\n",
    "            pl.plot(loss_log, '.', alpha=0.1)\n",
    "            pl.yscale('log')\n",
    "            pl.ylim(np.min(loss_log), loss_log[0])\n",
    "            pl.show()\n",
    "            \n",
    "            # * show batch\n",
    "            after = x.detach().cpu()\n",
    "            show_batch(_BATCH_SIZE_, before, after)\n",
    "            \n",
    "            # * print info\n",
    "            print('\\rstep:', i, '\\tloss:', loss.item(), '\\tlr:', lr_sched.get_last_lr()[0], end='')\n",
    "                \n",
    "        # * save checkpoint\n",
    "        if i % _SAVE_RATE_ == 0 and i != 0:\n",
    "            save_model('checkpoints', model, _NAME_+'_cp'+str(i))\n",
    "            \n",
    "# * save final model\n",
    "if _TRAIN_MODEL_:\n",
    "    save_model('models', model, _NAME_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.1 (SDL 2.28.2, Python 3.11.3)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "perception channels:  48\n",
      "hidden channels:  128\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import datetime\n",
    "import json\n",
    "\n",
    "_PLAY_ = False\n",
    "_PLAY_MODEL_ = 'models\\\\sobel_mag_strawbby_2_seeds_v1'\n",
    "_PLAY_DEVICE_ = 'cpu'\n",
    "_RADIUS_ = 8\n",
    "_SCALE_WINDOW_ = 10\n",
    "\n",
    "# * set current device\n",
    "_DEVICE_ = _PLAY_DEVICE_\n",
    "\n",
    "params = {}\n",
    "with open(_PLAY_MODEL_+'_params.json', 'r') as openfile:\n",
    "    # Reading from json file\n",
    "    params = json.load(openfile)\n",
    "\n",
    "# set kernel dividers\n",
    "_SOBEL_DIV_ = params['_SOBEL_DIV_']\n",
    "_LAP_DIV_ = params['_LAP_DIV_']\n",
    "    \n",
    "model = ISO_NCA(_device=_PLAY_DEVICE_)\n",
    "model.load_state_dict(torch.load(_PLAY_MODEL_+'.pt', map_location=_PLAY_DEVICE_))\n",
    "model.eval()\n",
    "\n",
    "# * misc params\n",
    "c = 0\n",
    "angle = 0.0\n",
    "fps = 0\n",
    "prev_time = datetime.datetime.now()\n",
    "\n",
    "# * create seed and tensor\n",
    "seed_img = load_image_as_tensor('..\\\\_seeds\\\\'+params['_SEED_FILE_'], params['_SIZE_'])\n",
    "seed_img_rot = trans.rotate(seed_img, np.rad2deg(params['_SEED_ANGLE_RAD_']+angle))\n",
    "tensor = torch.cat([seed_img_rot, torch.zeros([1, params['_CHANNELS_']-4, params['_SIZE_'], params['_SIZE_']])], 1).to(_PLAY_DEVICE_)\n",
    "\n",
    "# * start pygame\n",
    "pygame.init()\n",
    "pygame.display.set_caption('nca play - '+_PLAY_MODEL_)\n",
    "\n",
    "# * model dependent params\n",
    "size = params['_SIZE_']\n",
    "window_size = size * _SCALE_WINDOW_\n",
    "window = pygame.display.set_mode((window_size, window_size))\n",
    "\n",
    "# * text renders\n",
    "font_size = 24\n",
    "font_color = (255, 255, 255)\n",
    "my_font = pygame.font.SysFont('consolas', font_size)\n",
    "model_surface = my_font.render('model: ' + _PLAY_MODEL_, False, font_color)\n",
    "text_surface = my_font.render('angle: ' + str(angle) + 'π', False, font_color)\n",
    "fps_surface = my_font.render('fps: ' + str(int(fps)), False, font_color)\n",
    "\n",
    "# * start infinite game loop\n",
    "running = True\n",
    "mouse_down = False\n",
    "model_start = False\n",
    "while running:\n",
    "    if not _PLAY_:\n",
    "        break\n",
    "    # empty cache\n",
    "    torch.cuda.empty_cache()\n",
    "    # handle events\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            running = False\n",
    "        if event.type == pygame.KEYDOWN:\n",
    "            if event.key == pygame.K_SPACE:\n",
    "                model_start = True\n",
    "            if event.key == pygame.K_r:\n",
    "                # * reset to rotated seed\n",
    "                model_start = False\n",
    "                seed_img_rot = trans.rotate(seed_img, np.rad2deg(params['_SEED_ANGLE_RAD_']+angle))\n",
    "                tensor = torch.cat([seed_img_rot, torch.zeros([1, params['_CHANNELS_']-4, params['_SIZE_'], params['_SIZE_']])], 1).to(_PLAY_DEVICE_)\n",
    "            # if event.key == pygame.K_UP:\n",
    "            #     curr += 1\n",
    "            #     if curr >= len(model_list):\n",
    "            #         curr = 0\n",
    "            # if event.key == pygame.K_DOWN:\n",
    "            #     curr -= 1\n",
    "            #     if curr < 0:\n",
    "            #         curr = len(model_list)-1\n",
    "            # if event.key == pygame.K_UP or event.key == pygame.K_DOWN:\n",
    "            #     # load new model\n",
    "            #     model, tensor, params = load_model(model_list[curr], args.models, device)\n",
    "            #     p = params['pad']\n",
    "            #     pygame.display.set_caption('nca play - ' + model_list[curr])\n",
    "            #     size = params['size'] + (2 * p)\n",
    "            #     scale = args.scale\n",
    "            #     window_size = size * scale\n",
    "            #     window = pygame.display.set_mode((window_size, window_size))\n",
    "            #     model_surface = my_font.render('model: ' + model_list[curr], False, font_color)\n",
    "        if event.type == pygame.MOUSEWHEEL:\n",
    "            # * let player rotate seed before starting model\n",
    "            if not model_start:\n",
    "                angle = np.round((event.y * 0.05) + angle, decimals=2)\n",
    "                text_surface = my_font.render('angle: ' + str(angle) + 'π', False, font_color)\n",
    "                seed_img_rot = trans.rotate(seed_img, np.rad2deg(params['_SEED_ANGLE_RAD_']+angle))\n",
    "                tensor = torch.cat([seed_img_rot, torch.zeros([1, params['_CHANNELS_']-4, params['_SIZE_'], params['_SIZE_']])], 1).to(_PLAY_DEVICE_)\n",
    "        if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "            mouse_down = True\n",
    "            if pygame.mouse.get_pressed(3)[2] and model_start:\n",
    "                mouse = np.array(pygame.mouse.get_pos(), dtype=float)\n",
    "                pos = mouse / window_size * size\n",
    "                dot = np.zeros_like(tensor.detach().cpu().numpy())\n",
    "                dot[:, 3:, int(pos[1]), int(pos[0])] = 1.0\n",
    "                tensor += torch.tensor(dot).to(_PLAY_DEVICE_)\n",
    "        if event.type == pygame.MOUSEBUTTONUP:\n",
    "            mouse_down = False\n",
    "        if (event.type == pygame.MOUSEMOTION or event.type == pygame.MOUSEBUTTONDOWN) and mouse_down and model_start:\n",
    "            mouse = np.array(pygame.mouse.get_pos(), dtype=float)\n",
    "            pos = mouse / window_size * size\n",
    "            if pygame.mouse.get_pressed(3)[0]:\n",
    "                mask = circle_mask(size, _RADIUS_, pos)\n",
    "                tensor[0:] *= torch.tensor(mask).to(_PLAY_DEVICE_)\n",
    "            \n",
    "    # * update tensor\n",
    "    if model_start:\n",
    "        with torch.no_grad():\n",
    "            tensor = model(tensor)\n",
    "    \n",
    "    # * draw tensor to window\n",
    "    window.fill((255, 255, 255))\n",
    "    img = to_rgb(tensor, _alpha='BLACK').squeeze()\n",
    "    #vis = img\n",
    "    vis = (np.array(img.cpu())*255).astype(np.uint8)\n",
    "    pixel = pygame.Surface((_SCALE_WINDOW_, _SCALE_WINDOW_))\n",
    "    for j in range(size):\n",
    "        for i in range(size):\n",
    "            color = vis[:, i, j]\n",
    "            pixel.fill(color)\n",
    "            draw_me = pygame.Rect(j*_SCALE_WINDOW_, i*_SCALE_WINDOW_, _SCALE_WINDOW_, _SCALE_WINDOW_)\n",
    "            window.blit(pixel, draw_me)\n",
    "    \n",
    "    # * calculate fps\n",
    "    now = datetime.datetime.now()\n",
    "    if (now - prev_time).seconds >= 1.0:\n",
    "        prev_time = now\n",
    "        fps_surface = my_font.render('fps: ' + str(int(fps)), False, font_color)\n",
    "        fps = 0\n",
    "    else:\n",
    "        fps += 1       \n",
    "    \n",
    "    # * render text\n",
    "    window.blit(model_surface, (0, 0))\n",
    "    window.blit(text_surface, (window_size-font_size-150, window_size-font_size))\n",
    "    window.blit(fps_surface, (0, window_size-font_size))\n",
    "    \n",
    "    # * flip it!\n",
    "    pygame.display.flip()\n",
    "\n",
    "# * quit it!\n",
    "pygame.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
