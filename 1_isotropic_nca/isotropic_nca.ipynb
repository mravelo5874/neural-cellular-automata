{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility Functions\n",
    "- NOTE: tensors are organized as follows: [BATCH_SIZE, CHANNELS, WIDTH, HEIGHT]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce GTX 1660 Ti (UUID: GPU-d77bcb98-b41a-4494-1c93-f4f7d0b55d9c)\n",
      "cuda available?  True\n",
      "device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as func\n",
    "import PIL.Image\n",
    "import random\n",
    "import json\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import matplotlib.pylab as pl\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from torchvision.transforms.functional_tensor import gaussian_blur\n",
    "import torchvision.transforms.functional as trans\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "\n",
    "# * loads an image and converts to a tensor\n",
    "# *     default tensor shape: [BATCH_SIZE (1), CHANNELS (4), WIDTH (_size), HEIGHT (_size)]\n",
    "def load_image_as_tensor(_path, _size, _resample=PIL.Image.Resampling.BICUBIC):\n",
    "    img = PIL.Image.open(_path)\n",
    "    img = img.resize((_size, _size), _resample)\n",
    "    img = np.float32(img) / 255.0\n",
    "    img[..., :3] *= img[..., 3:]\n",
    "    return torch.from_numpy(img).permute(2, 0, 1)[None, ...]\n",
    "\n",
    "# * given a tensor of default shape, visualize the first 4 channels as a RGBA image\n",
    "def show_tensor_as_image(_tensor):\n",
    "    img = to_rgb(_tensor).squeeze().permute(1, 2, 0)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "# * takes the first 4 channels of a tensor of default shape and converts to a RGB image\n",
    "def to_rgb(_x, _alpha='BLACK'):\n",
    "    rgb, a = _x[:, :3], _x[:, 3:4]\n",
    "    if _alpha == 'BLACK':\n",
    "        return torch.clamp(rgb, 0.0, 1.0)\n",
    "    elif _alpha == 'WHITE':\n",
    "        return torch.clamp(1.-a + rgb, 0.0, 1.0)\n",
    "\n",
    "# * creates a circle mask centered at a position of a given radius\n",
    "def circle_mask(_size, _radius, _pos):\n",
    "    Y, X = np.ogrid[:_size, :_size]\n",
    "    dist_from_center = np.sqrt((X - _pos[0])**2 + (Y-_pos[1])**2)\n",
    "    mask = dist_from_center >= _radius\n",
    "    return mask\n",
    "\n",
    "# * creates a mask for half the screen\n",
    "def half_mask(_size, _type):\n",
    "    mask_types = ['left', 'right', 'top', 'bottom']\n",
    "    if _type == 'rand':\n",
    "        _type = mask_types[np.random.randint(0, 4)]\n",
    "    mat = np.zeros([_size, _size])\n",
    "    if _type == 'left':\n",
    "        mat[:, _size//2:] = 1.0\n",
    "    elif _type == 'right':\n",
    "        mat[:, :-_size//2] = 1.0\n",
    "    elif _type == 'top':\n",
    "        mat[_size//2:, :] = 1.0\n",
    "    elif _type == 'bottom':\n",
    "        mat[:-_size//2, :] = 1.0\n",
    "    return mat\n",
    "\n",
    "# * shows a batch before and after a forward pass given two (2) tensors\n",
    "def show_batch(_batch_size, _before, _after, _dpi=256):\n",
    "    fig = plt.figure(figsize=(_batch_size, 2), dpi=_dpi)\n",
    "    axarr = fig.subplots(nrows=2, ncols=_batch_size)\n",
    "    gspec = gridspec.GridSpec(2, _batch_size)\n",
    "    gspec.update(wspace=0.1, hspace=0) # set the spacing between axes.\n",
    "    plt.clf()\n",
    "    for i in range(_batch_size):\n",
    "        img_i = _before[i].unsqueeze(0)\n",
    "        img_rgb = to_rgb(img_i, _alpha='BLACK').squeeze().permute(1, 2, 0)\n",
    "        axarr[0, i] = plt.subplot(gspec[i])\n",
    "        axarr[0, i].set_xticks([])\n",
    "        axarr[0, i].set_yticks([])\n",
    "        axarr[0, i].imshow(img_rgb, aspect='equal')\n",
    "        axarr[0, i].set_title(str(i), fontsize=8)   \n",
    "    for i in range(_batch_size):\n",
    "        img_i = _after[i].unsqueeze(0)\n",
    "        img_rgb = to_rgb(img_i, _alpha='BLACK').squeeze().permute(1, 2, 0)\n",
    "        axarr[1, i] = plt.subplot(gspec[i+_batch_size])\n",
    "        axarr[1, i].set_xticks([])\n",
    "        axarr[1, i].set_yticks([])\n",
    "        axarr[1, i].imshow(img_rgb, aspect='equal') \n",
    "    plt.show()\n",
    "\n",
    "# * find GPU available\n",
    "clear_output()\n",
    "!nvidia-smi -L\n",
    "\n",
    "# * sets the device\n",
    "# *     defaults to 'cuda'\n",
    "_DEVICE_ = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print ('cuda available? ', torch.cuda.is_available())\n",
    "print ('device: ', _DEVICE_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random half mask:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# experiment block\n",
    "\n",
    "img = load_image_as_tensor('..\\\\_images\\\\'+'earth.png', 40)\n",
    "img = func.pad(img, (12, 12, 12, 12), 'constant', 0)\n",
    "\n",
    "mask = half_mask(64, 'rand')\n",
    "img = img * torch.tensor(mask, dtype=torch.int64)\n",
    "print ('random half mask:')\n",
    "show_tensor_as_image(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Pre-Made Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed_img.shape:  torch.Size([1, 4, 64, 64])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAFJ0lEQVR4nO3bsYrDMBRFQWnx//+y0iyHLbJgQhyLMFOaFLc7PETmWmsNABhj/Nw9AIB9iAIAEQUAIgoARBQAiCgAEFEAIKIAQI6zP5xzXrkDgIud+a+ySwGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgx90DYBfrn+/zoyvgXi4FACIKAEQUAIgoABAPzfDLgzK4FAD4QxQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAOe4eAO+xnnybH1/xdMYYt0yBV7gUAIgoABBRACCiAEA8NPMlNnnJ3WQGvMqlAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoAJDj7A/XWlfuAGADLgUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFAPIAkJUQEswAUWgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAFEElEQVR4nO3ZMU5EMRAFwTHa+195iGgCNkII/6AqdPSy1shnd3cAYGY+bg8A4DlEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgr9sD4DHOef+++7874CKXAgARBQAiCgBEFACIj2b44kMZXAoAfBMFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQF63B8BfOOfn2+6FHfNmyMzsXBgDv+BSACCiAEBEAYCIAgA5uze+4wB4IpcCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgD5BPBPEwVBNepHAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_SEED_FILE_ = '_3_seeds_64.png'\n",
    "_SIZE_ = 64\n",
    "_SEED_ANGLE_RAD_ = (0)*np.pi\n",
    "\n",
    "seed_img = load_image_as_tensor('..\\\\_seeds\\\\'+_SEED_FILE_,  _SIZE_)\n",
    "seed_img = trans.rotate(seed_img, np.rad2deg(_SEED_ANGLE_RAD_))\n",
    "print ('seed_img.shape: ', seed_img.shape)\n",
    "show_tensor_as_image(to_rgb(seed_img))\n",
    "show_tensor_as_image(to_rgb(seed_img, 'WHITE'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target Image to Train Model\n",
    "- NOTE: seed _SIZE_ should equal _TARGET-SIZE_ + (2 * _PAD_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_img.shape:  torch.Size([1, 4, 64, 64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_TARGET_FILE_ = 'earth.png'\n",
    "_TARGET_SIZE_ = 64\n",
    "_PAD_ = 0\n",
    "\n",
    "target_img = load_image_as_tensor('..\\\\_images\\\\'+_TARGET_FILE_, _TARGET_SIZE_)\n",
    "target_img = func.pad(target_img, (_PAD_, _PAD_, _PAD_, _PAD_), 'constant', 0)\n",
    "print ('target_img.shape: ', target_img.shape)\n",
    "show_tensor_as_image(target_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptions:\n",
    "- LAPLACIAN: isotropic nca model\n",
    "- SOBEL_MAG: isotrpic nca variant which adds upon the 'laplacian' model by making use of the magnitude of the two directional sobel filters\n",
    "- ANGLE_STEER: angle-based steerable nca\n",
    "- GRAD_STEER: gradient-based steerable nca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_SOBEL_DIV_ = 8.0\n",
    "_LAP_DIV_ = 12.0\n",
    "\n",
    "SOBEL_KERN = torch.tensor([\n",
    "    [-1., 0., 1.], \n",
    "    [-2., 1., 2.], \n",
    "    [-1., 0., 1.]])\n",
    "LAP_KERN = torch.tensor([\n",
    "    [1.,   2., 1.], \n",
    "    [2., -12., 2.], \n",
    "    [1.,   2., 1.]])\n",
    "ID_KERN = torch.tensor([\n",
    "    [0., 0., 0.], \n",
    "    [0., 1., 0.], \n",
    "    [0., 0., 0.]])\n",
    "\n",
    "# * performs a convolution per filter per channel\n",
    "def per_channel_conv(_x, _filters):\n",
    "    batch_size, channels, height, width = _x.shape\n",
    "    # * reshape x to make per-channel convolution possible + pad 1 on each side\n",
    "    y = _x.reshape(batch_size*channels, 1, height, width)\n",
    "    y = func.pad(y, (1, 1, 1, 1), 'circular')\n",
    "    # send to current device\n",
    "    _filters = _filters.to(_DEVICE_)\n",
    "    y = y.to(_DEVICE_)\n",
    "    # * perform per-channel convolutions\n",
    "    y = func.conv2d(y, _filters[:, None])\n",
    "    y = y.reshape(batch_size, -1, height, width)\n",
    "    return y\n",
    "\n",
    "# * only uses laplacian operator for local perception\n",
    "def laplacian_perception(_x):\n",
    "    # * add an extra dimention to account for batch size\n",
    "    lap_conv = per_channel_conv(_x, LAP_KERN[None, :]/_LAP_DIV_)\n",
    "    # * concat perception w/ self (identity)\n",
    "    y = torch.cat([_x, lap_conv], 1)\n",
    "    return y\n",
    "\n",
    "# * uses laplacian operator and sobel-magnitude (G) for local perception\n",
    "def sobel_mag_perception(_x):\n",
    "    # * add an extra dimention to account for batch size\n",
    "    lap_conv = per_channel_conv(_x, LAP_KERN[None, :]/_LAP_DIV_)\n",
    "    # * compute sobel-magnitude (G)\n",
    "    sobel_conv = per_channel_conv(_x, torch.stack([SOBEL_KERN/_SOBEL_DIV_, SOBEL_KERN.T/_SOBEL_DIV_]))\n",
    "    gx, gy = sobel_conv[:, ::2], sobel_conv[:, 1::2]\n",
    "    # * concat perceptions w/ self (identity)\n",
    "    y = torch.cat([_x, lap_conv, (gx*gx+gy*gy+1e-8).sqrt()], 1)\n",
    "    return y\n",
    "\n",
    "def angle_steerable_perception(_x):\n",
    "    # * separate states and angle channels\n",
    "    states, angle = _x[:, :-1], _x[:, -1:]\n",
    "    # * compute lap, gx and gy\n",
    "    lap_conv = per_channel_conv(states, LAP_KERN[None, :]/_LAP_DIV_)\n",
    "    gx = per_channel_conv(states, SOBEL_KERN[None, :]/_SOBEL_DIV_)\n",
    "    gy = per_channel_conv(states, SOBEL_KERN.T[None, :]/_SOBEL_DIV_)\n",
    "    # * compute px and py \n",
    "    _cos, _sin = angle.cos(), angle.sin()\n",
    "    px = (gx*_cos)+(gy*_sin)\n",
    "    py = (gy*_cos)-(gx*_sin)\n",
    "    # * concat and return\n",
    "    y = torch.cat([states, lap_conv, px, py], 1)\n",
    "    return y\n",
    "\n",
    "def gradient_steerable_perception(_x):\n",
    "    # * compute sobel x/y convolutions\n",
    "    filters = torch.stack([SOBEL_KERN/_SOBEL_DIV_, SOBEL_KERN.T/_SOBEL_DIV_])\n",
    "    grad = per_channel_conv(_x, filters)\n",
    "    # * extract grad and dir\n",
    "    grad, dir = grad[:, :-2], grad[:, -2:]\n",
    "    dir = dir / dir.norm(dim=1, keepdim=True).clip(1.0)\n",
    "    gx, gy = grad[:, ::2], grad[:, 1::2]\n",
    "    # * rotate gx and gy using sin/cos of dir\n",
    "    _cos, _sin = dir[:, :1], dir[:, 1::2]\n",
    "    rot_grad = torch.cat([gx*_cos+gy*_sin, gy*_cos-gx*_sin], 1)\n",
    "    lap_conv = per_channel_conv(_x, LAP_KERN[None, :])\n",
    "    # * concat and return\n",
    "    y = torch.cat([_x, lap_conv, rot_grad], 1)\n",
    "    return y\n",
    "    \n",
    "perception = {\n",
    "    'LAPLACIAN': laplacian_perception,\n",
    "    'SOBEL_MAG': sobel_mag_perception,\n",
    "    'ANGLE_STEER': angle_steerable_perception,\n",
    "    'GRAD_STEER': gradient_steerable_perception,\n",
    "}\n",
    "\n",
    "def get_alive_mask(_x):\n",
    "    return func.max_pool2d(_x[:, 3:4, :, :], kernel_size=3, stride=1, padding=1) > 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "states.shape: torch.Size([1, 3, 3, 3])\n",
      "states:\n",
      " tensor([[[[6., 4., 5.],\n",
      "          [5., 6., 2.],\n",
      "          [4., 2., 7.]],\n",
      "\n",
      "         [[3., 6., 5.],\n",
      "          [3., 8., 0.],\n",
      "          [2., 4., 4.]],\n",
      "\n",
      "         [[7., 6., 7.],\n",
      "          [3., 6., 2.],\n",
      "          [1., 4., 2.]]]], device='cuda:0')\n",
      "--------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 6.0000e+00,  4.0000e+00,  5.0000e+00],\n",
       "          [ 5.0000e+00,  6.0000e+00,  2.0000e+00],\n",
       "          [ 4.0000e+00,  2.0000e+00,  7.0000e+00]],\n",
       "\n",
       "         [[ 3.0000e+00,  6.0000e+00,  5.0000e+00],\n",
       "          [ 3.0000e+00,  8.0000e+00,  0.0000e+00],\n",
       "          [ 2.0000e+00,  4.0000e+00,  4.0000e+00]],\n",
       "\n",
       "         [[ 7.0000e+00,  6.0000e+00,  7.0000e+00],\n",
       "          [ 3.0000e+00,  6.0000e+00,  2.0000e+00],\n",
       "          [ 1.0000e+00,  4.0000e+00,  2.0000e+00]],\n",
       "\n",
       "         [[-1.9000e+01,  8.0000e+00, -5.0000e+00],\n",
       "          [-6.0000e+00, -2.4000e+01,  3.8000e+01],\n",
       "          [ 9.0000e+00,  3.6000e+01, -3.7000e+01]],\n",
       "\n",
       "         [[ 1.2000e+01, -2.3000e+01, -1.7000e+01],\n",
       "          [ 9.0000e+00, -5.6000e+01,  5.5000e+01],\n",
       "          [ 2.3000e+01,  3.0000e+00, -6.0000e+00]],\n",
       "\n",
       "         [[-3.6000e+01, -1.6000e+01, -3.6000e+01],\n",
       "          [ 1.5000e+01, -2.5000e+01,  3.0000e+01],\n",
       "          [ 4.1000e+01,  1.0000e+00,  2.6000e+01]],\n",
       "\n",
       "         [[ 9.1019e-01,  1.0062e+00,  3.2890e-01],\n",
       "          [ 4.2500e-01, -9.7129e-02, -4.0964e-01],\n",
       "          [ 5.7366e-01,  4.1283e-01,  8.7781e-01]],\n",
       "\n",
       "         [[ 1.5268e+00,  1.7330e+00, -2.1926e-02],\n",
       "          [ 1.7000e+00,  1.4569e-01,  1.2436e+00],\n",
       "          [ 1.2248e+00,  8.9745e-01,  1.7556e+00]],\n",
       "\n",
       "         [[ 2.7893e-01,  2.2361e-01, -1.2060e+00],\n",
       "          [ 7.7500e-01,  3.3995e-01,  3.8038e-01],\n",
       "          [ 7.5971e-01, -3.7693e-01, -1.9312e+00]],\n",
       "\n",
       "         [[-8.3679e-01, -3.9131e-01,  1.0305e+00],\n",
       "          [ 1.8500e+00,  7.7703e-01, -1.5362e+00],\n",
       "          [-8.8375e-01, -5.9232e-01,  4.0964e-01]]]], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# * experiments\n",
    "states = torch.randint(0, 9, [1, 3, 3, 3]).to(_DEVICE_).to(torch.float32)\n",
    "print ('states.shape:', states.shape)\n",
    "print ('states:\\n', states)\n",
    "print ('--------')\n",
    "gradient_steerable_perception(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angle 0.478135652282558\n",
      "states.shape: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[6., 0., 0.],\n",
      "          [7., 8., 4.],\n",
      "          [2., 1., 8.]]]], device='cuda:0')\n",
      "normal sobel-x conv:\n",
      "x: tensor([[[[ 3., -9., 12.],\n",
      "          [ 8.,  2.,  9.],\n",
      "          [-8.,  4., 15.]]]], device='cuda:0')\n",
      "pre-rot sobel-x conv:\n",
      "y: tensor([[[[ -6.0788, -14.8926,   8.8138],\n",
      "          [  3.4218,  -4.6660,   0.1686],\n",
      "          [ -1.5813,  12.7539,  17.4589]]]], device='cuda:0')\n",
      "post-rot sobel-x conv:\n",
      "z: tensor([[[[ -6.0788, -14.8926,   8.8138],\n",
      "          [  3.4218,  -4.6660,   0.1686],\n",
      "          [ -1.5813,  12.7539,  17.4589]]]], device='cuda:0')\n",
      "y=z?: tensor([[[[False,  True, False],\n",
      "          [False, False, False],\n",
      "          [False, False,  True]]]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marco\\AppData\\Local\\Temp\\ipykernel_7960\\939467357.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  rdx = torch.tensor(_cos*dx-_sin*dy, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# * experiments and tests\n",
    "angle = np.random.random()*np.pi\n",
    "states = torch.randint(0, 9, [1, 1, 3, 3]).to(_DEVICE_).to(torch.float32)\n",
    "print ('angle',angle)\n",
    "print ('states.shape:',states.shape)\n",
    "print (states)\n",
    "\n",
    "dx = SOBEL_KERN\n",
    "dy = SOBEL_KERN.T\n",
    "\n",
    "# normal sobel-x convolution\n",
    "print ('normal sobel-x conv:')\n",
    "x = per_channel_conv(states, dx[None, :])\n",
    "print ('x:', x)\n",
    "\n",
    "# pre-rotated sobel-x convolution\n",
    "print ('pre-rot sobel-x conv:')\n",
    "_cos, _sin = np.cos(angle), np.sin(angle)\n",
    "rdx = torch.tensor(_cos*dx-_sin*dy, dtype=torch.float32)\n",
    "y = per_channel_conv(states, rdx[None, :])\n",
    "print ('y:', y)\n",
    "\n",
    "# post-rotated sobel-x convolution\n",
    "print ('post-rot sobel-x conv:')\n",
    "gx = per_channel_conv(states, dx[None, :])\n",
    "gy = per_channel_conv(states, dy[None, :])\n",
    "# * compute px and py\n",
    "z = (gx*_cos)-(gy*_sin)\n",
    "zy = (gx*_cos)+(gy*_sin)\n",
    "print ('z:', z)\n",
    "\n",
    "print ('y=z?:',torch.eq(y,z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isotropic Neural Cellular Automata Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISO-NCA param count: 9856\n",
      "_IS_STEERABLE_: True\n"
     ]
    }
   ],
   "source": [
    "_CHANNELS_ = 16\n",
    "_HIDDEN_ = 128\n",
    "_MODEL_TYPE_ = 'ANGLE_STEER'  #['LAPLACIAN', 'SOBEL_MAG', 'ANGLE_STEER', 'GRAD_STEER']\n",
    "_IS_STEERABLE_ = True if 'ANGLE_STEER' in _MODEL_TYPE_ else False\n",
    "_STOCHASTIC_UPDATE_RATE_ = 0.5\n",
    "\n",
    "class ISO_NCA(torch.nn.Module):\n",
    "    def __init__(self, _channels=_CHANNELS_, _hidden=_HIDDEN_, _device=_DEVICE_, _model_type=_MODEL_TYPE_):\n",
    "        super().__init__()\n",
    "        self.device = _device\n",
    "        _MODEL_TYPE_=_model_type\n",
    "\n",
    "        # * determine number of perceived channels\n",
    "        perception_channels = perception[_MODEL_TYPE_](torch.zeros([1, _channels, 8, 8]).to(_device)).shape[1]\n",
    "        \n",
    "        # * determine hidden channels (equalize the parameter count btwn model types)\n",
    "        hidden_channels = 8*1024 // (perception_channels+_channels)\n",
    "        hidden_channels = (_hidden+31) // 32*32\n",
    "        \n",
    "        # * model layers\n",
    "        self.conv1 = torch.nn.Conv2d(perception_channels, hidden_channels, 1)\n",
    "        self.conv2 = torch.nn.Conv2d(hidden_channels, _channels, 1, bias=False)\n",
    "        with torch.no_grad():\n",
    "            self.conv2.weight.data.zero_()\n",
    "        \n",
    "        # * send to device\n",
    "        self.to(_device)\n",
    "        \n",
    "    def forward(self, _x):\n",
    "        # * get alive mask\n",
    "        alive_mask = get_alive_mask(_x).to(self.device)\n",
    "        \n",
    "        # * perception step\n",
    "        _x = _x.to(self.device)\n",
    "        p = perception[_MODEL_TYPE_](_x)\n",
    "        \n",
    "        # * update step\n",
    "        p = self.conv2(torch.relu(self.conv1(p)))\n",
    "        \n",
    "        # * create stochastic update mask\n",
    "        stochastic_mask = (torch.rand(_x[:, :1, :, :].shape) <= _STOCHASTIC_UPDATE_RATE_).to(self.device, torch.float32)\n",
    "        \n",
    "        # * perform update\n",
    "        _x = _x + p * stochastic_mask\n",
    "        if _IS_STEERABLE_:\n",
    "            states = _x[:, :-1]*alive_mask\n",
    "            angle = _x[:, -1:] % (np.pi*2.0)\n",
    "            _x = torch.cat([states, angle], 1)\n",
    "        else:\n",
    "            _x = _x * alive_mask\n",
    "        return _x\n",
    "\n",
    "# * print model parameter count\n",
    "param_n = sum(p.numel() for p in ISO_NCA().parameters())\n",
    "print('ISO-NCA param count:', param_n)\n",
    "\n",
    "print ('_IS_STEERABLE_: '+str(_IS_STEERABLE_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_LOSS_FUNC_ = 'PIXEL_WISE' #['PIXEL_WISE', 'INVARIANT']\n",
    "\n",
    "# r = torch.linspace(0.5/_SIZE_, 1, _SIZE_//2.0)[:, None]\n",
    "# a = torch.range(0, _SIZE_*np.pi)/(_SIZE_/2)\n",
    "# polar_xy = torch.stack([r*a.cos(), r*a.sin()], -1)[None, :]\n",
    "# polar_target = func.grid_sample(unsharpen(target_img[None, ...]), polar_xy)\n",
    "\n",
    "# x = torch.linspace(-1, 1, _SIZE_)\n",
    "# y, x = torch.meshgrid(x, x)\n",
    "# xy_grid = torch.stack([x, y], -1)\n",
    "# fft_target = torch.fft.rfft(polar_target).conj()\n",
    "# polar_target_sqnorm = polar_target.square().sum(-1, keepdim=True)\n",
    "\n",
    "def pixel_wise_loss_func(_x, _target, _scale=1e3, _dims=[]):\n",
    "    return _scale * torch.mean(torch.square(_x[:, :4] - _target), _dims)\n",
    "\n",
    "# def invariant_losses_func(_x):\n",
    "#     img = unsharpen(_x)\n",
    "#     polar_img = func.grid_sample(img, polar_xy.repeat(len(img), 1, 1, 1), mode='bicubic')\n",
    "#     x = torch.fft.rfft(polar_img)\n",
    "#     xy = torch.fft.irfft(x*fft_target)\n",
    "#     xx = polar_img.square().sum(-1, keepdim=True)\n",
    "#     yy = polar_target_sqnorm\n",
    "#     diff = xx+yy-2.0*xy\n",
    "#     return diff.mean([1, 2])\n",
    "\n",
    "def invariant_loss_func(_x):\n",
    "    raise NotImplementedError\n",
    "    # return invariant_losses_func(_x).min(-1)[0].mean()\n",
    "\n",
    "loss_func = {\n",
    "    'PIXEL_WISE': pixel_wise_loss_func,\n",
    "    'INVARIANT': invariant_loss_func\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create ISO-NCA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "_NAME_ = 'gard_steer_earth_3_seeds_v1'\n",
    "_POOL_SIZE_ = 256\n",
    "_BATCH_SIZE_ = 8\n",
    "_LOWER_LR_ = 1e-5\n",
    "_UPPER_LR_ = 1e-3\n",
    "\n",
    "# * create model / optimizer / lr-scheduler\n",
    "model = ISO_NCA()\n",
    "opt = torch.optim.Adam(model.parameters(), _UPPER_LR_)\n",
    "lr_sched = torch.optim.lr_scheduler.CyclicLR(opt, _LOWER_LR_, _UPPER_LR_, step_size_up=2000, mode='triangular2', cycle_momentum=False)\n",
    "\n",
    "# * create target batch\n",
    "target_batch = target_img.clone().repeat(_BATCH_SIZE_, 1, 1, 1).to(_DEVICE_)\n",
    "\n",
    "# * create seed\n",
    "seed = torch.cat([seed_img, torch.zeros([1, _CHANNELS_-4, _SIZE_, _SIZE_])], 1).to(_DEVICE_)\n",
    "\n",
    "# * create training pool\n",
    "with torch.no_grad():\n",
    "    pool = seed.clone().repeat(_POOL_SIZE_, 1, 1, 1)\n",
    "    # * randomize angles for steerable models\n",
    "    if _IS_STEERABLE_:\n",
    "        for i in range(_POOL_SIZE_):\n",
    "            rand = torch.rand(_SIZE_, _SIZE_)*np.pi*2.0\n",
    "            pool[i, -1:] = rand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load ISO-NCA Model Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = False\n",
    "load_from = 'checkpoints\\\\sobel_mag_cowboy_v2_cp14000'\n",
    "\n",
    "def load_model_checkpoint():\n",
    "    if not load_model: \n",
    "        return\n",
    "\n",
    "    # * open params json file\n",
    "    params = {}\n",
    "    with open(load_from + '_params.json', 'r') as openfile:\n",
    "        params = json.load(openfile)\n",
    "    \n",
    "    # * load in params\n",
    "    _DEVICE_ = params['_DEVICE_']\n",
    "    _SEED_FILE_ = params['_SEED_FILE_']\n",
    "    _SIZE_ = params['_SIZE_']\n",
    "    _SEED_ANGLE_RAD_ = params['_SEED_ANGLE_RAD_']\n",
    "    _NAME_ = params['_NAME_']\n",
    "    _MODEL_TYPE_ = params['_MODEL_TYPE_']\n",
    "    _IS_STEERABLE_ = params['_IS_STEERABLE_']\n",
    "    _POOL_SIZE_ = params['_POOL_SIZE_']\n",
    "    _TARGET_FILE_ = params['_TARGET_FILE_']\n",
    "    _TARGET_SIZE_ = params['_TARGET_SIZE_']\n",
    "    _PAD_ = params['_PAD_']\n",
    "    _SOBEL_DIV_ = params['_SOBEL_DIV_']\n",
    "    _LAP_DIV_ = params['_LAP_DIV_']\n",
    "    _BATCH_SIZE_ = params['_BATCH_SIZE_']\n",
    "    _LOWER_LR_ = params['_LOWER_LR_']\n",
    "    _UPPER_LR_ = params['_UPPER_LR_']\n",
    "    _LOSS_FUNC_ = params['_LOSS_FUNC_']\n",
    "    \n",
    "    # * load state dictionary\n",
    "    model.load_state_dict(torch.load(load_from + '.pt', map_location=_DEVICE_))   \n",
    "    model.train()\n",
    "\n",
    "    # * load seed\n",
    "    seed_img = load_image_as_tensor('..\\\\_seeds\\\\'+_SEED_FILE_, _SIZE_)\n",
    "    seed_img = trans.rotate(seed_img, np.rad2deg(_SEED_ANGLE_RAD_))\n",
    "    seed = torch.cat([seed_img, torch.zeros([1, _CHANNELS_-4, _SIZE_, _SIZE_])], 1).to(_DEVICE_)\n",
    "    # * create training pool\n",
    "    with torch.no_grad():\n",
    "        pool = seed.clone().repeat(_POOL_SIZE_, 1, 1, 1)\n",
    "        # * randomize angles for steerable models\n",
    "        if _IS_STEERABLE_:\n",
    "            for i in range(_POOL_SIZE_):\n",
    "                rand = torch.rand(_SIZE_, _SIZE_)*np.pi*2.0\n",
    "                pool[i, -1:] = rand\n",
    "        \n",
    "    # * load target\n",
    "    target_img = load_image_as_tensor('..\\\\_images\\\\'+_TARGET_FILE_, _TARGET_SIZE_)\n",
    "    target_img = torch.nn.functional.pad(target_img, (_PAD_, _PAD_, _PAD_, _PAD_), 'constant', 0)\n",
    "    target_batch = target_img.clone().repeat(_BATCH_SIZE_, 1, 1, 1).to(_DEVICE_)\n",
    "    \n",
    "    # * setup loss function etc.\n",
    "    opt = torch.optim.Adam(model.parameters(), _UPPER_LR_)\n",
    "    lr_sched = torch.optim.lr_scheduler.CyclicLR(opt, _LOWER_LR_, _UPPER_LR_, step_size_up=2000, mode='triangular2', cycle_momentum=False)\n",
    "    \n",
    "    print ('model loaded in successfully')\n",
    "\n",
    "load_model_checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_TRAIN_MODEL_ = False\n",
    "_EPOCHS_ = 20_000\n",
    "_NUM_DAMG_ = 4\n",
    "_DAMG_RATE_ = 2\n",
    "_INFO_RATE_ = 20\n",
    "_SAVE_RATE_ = 1000\n",
    "\n",
    "# * save model method\n",
    "def save_model(_dir, _model, _name):\n",
    "    model_path = pathlib.Path(_dir)\n",
    "    model_path.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(_model.state_dict(), _dir + '\\\\' + _name + '.pt')\n",
    "    \n",
    "    # * save model parameters\n",
    "    dict = {\n",
    "        # seed info\n",
    "        '_SEED_FILE_': _SEED_FILE_,\n",
    "        '_SIZE_': _SIZE_,\n",
    "        '_SEED_ANGLE_RAD_':_SEED_ANGLE_RAD_,\n",
    "        # target image info\n",
    "        '_TARGET_FILE_': _TARGET_FILE_,\n",
    "        '_TARGET_SIZE_': _TARGET_SIZE_,\n",
    "        '_PAD_': _PAD_,\n",
    "        # kernel dividers\n",
    "        '_SOBEL_DIV_': _SOBEL_DIV_,\n",
    "        '_LAP_DIV_': _LAP_DIV_,\n",
    "        # model info\n",
    "        '_CHANNELS_': _CHANNELS_,\n",
    "        '_HIDDEN_': _HIDDEN_,\n",
    "        '_MODEL_TYPE_': _MODEL_TYPE_,\n",
    "        '_IS_STEERABLE_': _IS_STEERABLE_,\n",
    "        '_STOCHASTIC_UPDATE_RATE_': _STOCHASTIC_UPDATE_RATE_,\n",
    "        # loss/lr info\n",
    "        '_LOSS_FUNC_': _LOSS_FUNC_,\n",
    "        '_LOWER_LR_': _LOWER_LR_,\n",
    "        '_UPPER_LR_': _UPPER_LR_,\n",
    "        # training info\n",
    "        '_DEVICE_': _DEVICE_,\n",
    "        '_NAME_': _NAME_,\n",
    "        '_EPOCHS_': _EPOCHS_,\n",
    "        '_POOL_SIZE_': _POOL_SIZE_,\n",
    "        '_BATCH_SIZE_': _BATCH_SIZE_,\n",
    "        '_NUM_DAMG_': _NUM_DAMG_,\n",
    "        # training rate info\n",
    "        '_DAMG_RATE_': _DAMG_RATE_,\n",
    "        '_INFO_RATE_': _INFO_RATE_,\n",
    "        '_SAVE_RATE_': _SAVE_RATE_,\n",
    "    }\n",
    "    json_object = json.dumps(dict, indent=4)\n",
    "    with open(_dir + '\\\\' + _name + '_params.json', 'w') as outfile:\n",
    "        outfile.write(json_object)\n",
    "    print ('model + params saved!')\n",
    "\n",
    "loss_log = []\n",
    "progress = 0\n",
    "\n",
    "# * begin training \n",
    "for _ in tqdm(range(_EPOCHS_)):\n",
    "    if not _TRAIN_MODEL_:\n",
    "        print ('skipping training')\n",
    "        break\n",
    "    with torch.no_grad():\n",
    "        # * sample batch from pool\n",
    "        i = len(loss_log)\n",
    "        batch_idxs = np.random.choice(_POOL_SIZE_, _BATCH_SIZE_, replace=False)\n",
    "        x = pool[batch_idxs]\n",
    "        \n",
    "        # * re-order batch based on loss\n",
    "        loss_ranks = torch.argsort(loss_func[_LOSS_FUNC_](x, target_batch, _dims=[-2, -3, -1]), descending=True)\n",
    "        x = x[loss_ranks]\n",
    "        \n",
    "        # * re-add seed into batch\n",
    "        x[:1] = seed\n",
    "\n",
    "        # * randomize angles for steerable models\n",
    "        if _IS_STEERABLE_:\n",
    "            rand = torch.rand(_SIZE_, _SIZE_)*np.pi*2.0\n",
    "            x[:1, -1:] = rand\n",
    "            \n",
    "        # * damage lowest loss in batch\n",
    "        if i % _DAMG_RATE_ == 0:\n",
    "            # * use random half mask\n",
    "            if i % 10 == 0:\n",
    "                mask = half_mask(_SIZE_, 'rand')\n",
    "            # * use random circle mask\n",
    "            else:\n",
    "                radius = random.uniform(_SIZE_*0.05, _SIZE_*0.2)\n",
    "                u = random.uniform(0, 1) * _SIZE_\n",
    "                v = random.uniform(0, 1) * _SIZE_\n",
    "                mask = circle_mask(_SIZE_, radius, [u, v])\n",
    "            x[-_NUM_DAMG_:] *= torch.tensor(mask).to(_DEVICE_)\n",
    "            \n",
    "    # * different loss values\n",
    "    overflow_loss = 0.0\n",
    "    diff_loss = 0.0\n",
    "    target_loss = 0.0\n",
    "    \n",
    "    # * save batch before\n",
    "    if i % _INFO_RATE_ == 0:\n",
    "        before = x.detach().cpu()\n",
    "    \n",
    "    # * forward pass\n",
    "    num_steps = np.random.randint(64, 96)\n",
    "    for _ in range(num_steps):\n",
    "        prev_x = x\n",
    "        x = model(x)\n",
    "        diff_loss += (x - prev_x).abs().mean()\n",
    "        if _IS_STEERABLE_:\n",
    "            overflow_loss += (x - x.clamp(-2.0, 2.0))[:, :-1].square().sum()\n",
    "        else:\n",
    "            overflow_loss += (x - x.clamp(-2.0, 2.0))[:, ...].square().sum()\n",
    "    \n",
    "    # * calculate losses\n",
    "    target_loss += loss_func[_LOSS_FUNC_](x, target_batch)\n",
    "    target_loss /= 2.0\n",
    "    diff_loss *= 10.0\n",
    "    loss = target_loss + overflow_loss + diff_loss\n",
    "    \n",
    "    # * backward pass\n",
    "    with torch.no_grad():\n",
    "        loss.backward()\n",
    "        # * normalize gradients \n",
    "        for p in model.parameters():\n",
    "            p.grad /= (p.grad.norm()+1e-8) \n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        lr_sched.step()\n",
    "        # * re-add batch to pool\n",
    "        pool[batch_idxs] = x\n",
    "        loss_log.append(loss.item())\n",
    "        \n",
    "        # * print out info\n",
    "        if i % _INFO_RATE_ == 0:\n",
    "            # * show loss plot\n",
    "            clear_output(True)\n",
    "            pl.plot(loss_log, '.', alpha=0.1)\n",
    "            pl.yscale('log')\n",
    "            pl.ylim(np.min(loss_log), loss_log[0])\n",
    "            pl.show()\n",
    "            \n",
    "            # * show batch\n",
    "            after = x.detach().cpu()\n",
    "            show_batch(_BATCH_SIZE_, before, after)\n",
    "            \n",
    "            # * print info\n",
    "            print('\\rstep:', i, '\\tloss:', loss.item(), '\\tlr:', lr_sched.get_last_lr()[0], end='')\n",
    "                \n",
    "        # * save checkpoint\n",
    "        if i % _SAVE_RATE_ == 0 and i != 0:\n",
    "            save_model('checkpoints', model, _NAME_+'_cp'+str(i))\n",
    "            \n",
    "# * save final model\n",
    "if _TRAIN_MODEL_:\n",
    "    save_model('models', model, _NAME_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.1 (SDL 2.28.2, Python 3.11.3)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "seeds:  ['_1_seed_64.png', '_2x2_seeds_64.png', '_2x3_seeds_64.png', '_2_seeds_64.png', '_3_seeds_64.png']\n",
      "models:  ['angle_steer_cowboy_2_seeds_v2', 'angle_steer_cowboy_2_seeds_v3', 'angle_steer_cowboy_2_seeds_v4', 'gard_steer_earth_3_seeds_v1', 'grad_steer_cowboy_2_seeds_v1', 'laplacian_cowboy_v1', 'sobel_mag_bonzai_3_seeds_v1', 'sobel_mag_cowboy_2_seeds_v1', 'sobel_mag_cowboy_v1', 'sobel_mag_cowboy_v2', 'sobel_mag_rainbow_v1', 'sobel_mag_strawbby_2_seeds_v1']\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import imutils\n",
    "\n",
    "_PLAY_ = True\n",
    "_MODELS_DIR_ = 'models'\n",
    "_PLAY_DEVICE_ = 'cpu'\n",
    "_RADIUS_ = 8\n",
    "_PLAY_SIZE_ = 64\n",
    "_WINDOW_SCALE_ = 10\n",
    "_WINDOW_BG_COLOR_ = (0, 0, 0)\n",
    "_WINDOW_TEXT_COLOR_ = (255, 255, 255)\n",
    "\n",
    "# * set current device\n",
    "_DEVICE_ = _PLAY_DEVICE_\n",
    "\n",
    "# * method to load model for play\n",
    "def load_model(_model_name):\n",
    "    # * read params from json file\n",
    "    params = {}\n",
    "    with open(_MODELS_DIR_+'\\\\'+_model_name+'_params.json', 'r') as openfile:\n",
    "        params = json.load(openfile)\n",
    "\n",
    "    # * set important parameters\n",
    "    global _SOBEL_DIV_\n",
    "    global _LAP_DIV_ \n",
    "    global _MODEL_TYPE_\n",
    "    global _IS_STEERABLE_\n",
    "    global _LOSS_FUNC_\n",
    "    global _ANGLE_CHANNEL_\n",
    "    _SOBEL_DIV_ = params['_SOBEL_DIV_']\n",
    "    _LAP_DIV_ = params['_LAP_DIV_']\n",
    "    _MODEL_TYPE_ = params['_MODEL_TYPE_']\n",
    "    _IS_STEERABLE_ = params['_IS_STEERABLE_']\n",
    "    _LOSS_FUNC_ = params['_MODEL_TYPE_']\n",
    "    # * last state channel is angle and should be treated differently\n",
    "    num = 1 if _IS_STEERABLE_ else 0\n",
    "    _ANGLE_CHANNEL_ = _CHANNELS_ - num\n",
    "    \n",
    "    # * load in model\n",
    "    model = ISO_NCA(_device=_PLAY_DEVICE_, _model_type=_MODEL_TYPE_)\n",
    "    model.load_state_dict(torch.load(_MODELS_DIR_+'\\\\'+_model_name+'.pt', map_location=_PLAY_DEVICE_))\n",
    "    model.eval()\n",
    "\n",
    "    # * create seed and tensor\n",
    "    seed_img = load_image_as_tensor('..\\\\_seeds\\\\'+params['_SEED_FILE_'], params['_SIZE_'])\n",
    "    seed_img_rot = trans.rotate(seed_img, np.rad2deg(params['_SEED_ANGLE_RAD_']))\n",
    "    pad = np.round((_PLAY_SIZE_ - params['_SIZE_'])/2.0, 0).astype(int)\n",
    "    seed_pad = func.pad(seed_img_rot, (pad, pad, pad, pad), 'constant', 0)\n",
    "    tensor = torch.cat([seed_pad, torch.zeros([1, params['_CHANNELS_']-4, _PLAY_SIZE_, _PLAY_SIZE_])], 1).to(_PLAY_DEVICE_)\n",
    "    # * randomize angles for steerable models\n",
    "    if _IS_STEERABLE_:\n",
    "        rand = torch.rand(_PLAY_SIZE_, _PLAY_SIZE_)*np.pi*2.0\n",
    "        tensor[:, -1:] = rand\n",
    "    return model, tensor, params, seed_img\n",
    "\n",
    "# * get list of seeds\n",
    "seeds_list = os.listdir('..\\\\_seeds')\n",
    "seeds_list = [item for item in seeds_list if item.endswith('.png')]\n",
    "print ('seeds: ', seeds_list)\n",
    "curr_seed = 0\n",
    "\n",
    "# * get list of models\n",
    "model_list = os.listdir(_MODELS_DIR_)\n",
    "model_list = [item.replace('.pt', '') for item in model_list if item.endswith('.pt')]\n",
    "print ('models: ', model_list)\n",
    "curr_model = 0\n",
    "\n",
    "# load first model\n",
    "model, tensor, params, seed_img = load_model(model_list[curr_model])\n",
    "\n",
    "# * misc params\n",
    "angle = 0.0\n",
    "fps = 0\n",
    "show_vecs = False\n",
    "bg_color = 'WHITE' if _WINDOW_BG_COLOR_ == (255, 255, 255) else 'BLACK'\n",
    "prev_time = datetime.datetime.now()\n",
    "\n",
    "# * load vector image\n",
    "vec_img = cv2.imread('..\\\\_images\\\\vector_v3.png') \n",
    "vec_img = cv2.resize(vec_img, (_WINDOW_SCALE_, _WINDOW_SCALE_))\n",
    "vec_img = vec_img.astype(float)/255.0\n",
    "\n",
    "# * start pygame\n",
    "pygame.init()\n",
    "pygame.display.set_caption('nca play - '+params['_NAME_'])\n",
    "\n",
    "# * model dependent params\n",
    "size = _PLAY_SIZE_\n",
    "window_size = size * _WINDOW_SCALE_\n",
    "window = pygame.display.set_mode((window_size, window_size))\n",
    "\n",
    "# * text renders\n",
    "font_size = 20\n",
    "my_font = pygame.font.SysFont('consolas', font_size)\n",
    "model_surface = my_font.render('[UP/DOWN] model: ' + params['_NAME_'], False, _WINDOW_TEXT_COLOR_)\n",
    "seed_surface = my_font.render('[LEFT/RIGHT] seed: ' + params['_SEED_FILE_'], False, _WINDOW_TEXT_COLOR_)\n",
    "text_surface = my_font.render('[SCROLL] angle: ' + str(angle) + 'π', False, _WINDOW_TEXT_COLOR_)\n",
    "fps_surface = my_font.render('fps: ' + str(int(fps)), False, _WINDOW_TEXT_COLOR_)\n",
    "vec_surface = my_font.render('[V] show vectors: ' + str(show_vecs), False, _WINDOW_TEXT_COLOR_)\n",
    "\n",
    "# * start infinite game loop\n",
    "running = True\n",
    "mouse_down = False\n",
    "model_start = False\n",
    "while running:\n",
    "    if not _PLAY_:\n",
    "        print ('skipping game')\n",
    "        break\n",
    "    # empty cache\n",
    "    torch.cuda.empty_cache()\n",
    "    # handle events\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            running = False\n",
    "        if event.type == pygame.KEYDOWN:\n",
    "            # * close application\n",
    "            if event.key == pygame.K_ESCAPE:\n",
    "                running = False\n",
    "                break\n",
    "            # * toggle showing vectors\n",
    "            if event.key == pygame.K_v:\n",
    "                show_vecs = not show_vecs\n",
    "                vec_surface = my_font.render('[V] show vectors: ' + str(show_vecs), False, _WINDOW_TEXT_COLOR_)\n",
    "            # * start current model\n",
    "            if event.key == pygame.K_SPACE:\n",
    "                model_start = not model_start\n",
    "            # * reset current model\n",
    "            if event.key == pygame.K_r:\n",
    "                model_start = False\n",
    "                seed_img_rot = trans.rotate(seed_img, np.rad2deg(params['_SEED_ANGLE_RAD_']+(angle*np.pi)))\n",
    "                pad = np.round((_PLAY_SIZE_ - params['_SIZE_'])/2.0, 0).astype(int)\n",
    "                seed_pad = func.pad(seed_img_rot, (pad, pad, pad, pad), 'constant', 0)\n",
    "                tensor = torch.cat([seed_pad, torch.zeros([1, params['_CHANNELS_']-4, _PLAY_SIZE_, _PLAY_SIZE_])], 1).to(_PLAY_DEVICE_)\n",
    "                # * randomize angles for steerable models\n",
    "                if _IS_STEERABLE_:\n",
    "                    rand = torch.rand(_PLAY_SIZE_, _PLAY_SIZE_)*np.pi*2.0\n",
    "                    tensor[:, -1:] = rand\n",
    "            # * use up/down arrow keys to cycle though models\n",
    "            if event.key == pygame.K_UP:\n",
    "                    curr_model += 1\n",
    "                    if curr_model >= len(model_list):\n",
    "                        curr_model = 0\n",
    "            if event.key == pygame.K_DOWN:\n",
    "                curr_model -= 1\n",
    "                if curr_model < 0:\n",
    "                    curr_model = len(model_list)-1\n",
    "            # * load new model\n",
    "            if event.key == pygame.K_UP or event.key == pygame.K_DOWN:\n",
    "                model_start = False\n",
    "                model, tensor, params, seed_img = load_model(model_list[curr_model])\n",
    "                pygame.display.set_caption('nca play - '+params['_NAME_'])\n",
    "                model_surface = my_font.render('[UP/DOWN] model: '+params['_NAME_'], False, _WINDOW_TEXT_COLOR_)\n",
    "                seed_surface = my_font.render('[LEFT/RIGHT] seed: '+params['_SEED_FILE_'], False, _WINDOW_TEXT_COLOR_)\n",
    "            # * use left/right arrow keys to cycle though seeds\n",
    "            if event.key == pygame.K_LEFT:\n",
    "                curr_seed += 1\n",
    "                if curr_seed >= len(seeds_list):\n",
    "                    curr_seed = 0\n",
    "            if event.key == pygame.K_RIGHT:\n",
    "                curr_seed -= 1\n",
    "                if curr_seed < 0:\n",
    "                    curr_seed = len(seeds_list)-1\n",
    "            # * load new seed image\n",
    "            if event.key == pygame.K_LEFT or event.key == pygame.K_RIGHT:\n",
    "                model_start = False\n",
    "                seed_img = load_image_as_tensor('..\\\\_seeds\\\\'+seeds_list[curr_seed], params['_SIZE_'])\n",
    "                seed_img_rot = trans.rotate(seed_img, np.rad2deg(params['_SEED_ANGLE_RAD_']))\n",
    "                pad = np.round((_PLAY_SIZE_ - params['_SIZE_'])/2.0, 0).astype(int)\n",
    "                seed_pad = func.pad(seed_img_rot, (pad, pad, pad, pad), 'constant', 0)\n",
    "                tensor = torch.cat([seed_pad, torch.zeros([1, params['_CHANNELS_']-4, _PLAY_SIZE_, _PLAY_SIZE_])], 1).to(_PLAY_DEVICE_)\n",
    "                # * randomize angles for steerable models\n",
    "                if _IS_STEERABLE_:\n",
    "                    rand = torch.rand(_PLAY_SIZE_, _PLAY_SIZE_)*np.pi*2.0\n",
    "                    tensor[:, -1:] = rand\n",
    "                seed_surface = my_font.render('[LEFT/RIGHT] seed: '+seeds_list[curr_seed], False, _WINDOW_TEXT_COLOR_)\n",
    "        if event.type == pygame.MOUSEWHEEL:\n",
    "            # * let player rotate seed before starting model\n",
    "            if not model_start:\n",
    "                angle = np.round((event.y * 0.05) + angle, decimals=2)\n",
    "                text_surface = my_font.render('[SCROLL] angle: ' + str(angle) + 'π', False, _WINDOW_TEXT_COLOR_)\n",
    "                seed_img_rot = trans.rotate(seed_img, np.rad2deg(params['_SEED_ANGLE_RAD_']+(angle*np.pi)))\n",
    "                pad = np.round((_PLAY_SIZE_ - params['_SIZE_'])/2.0, 0).astype(int)\n",
    "                seed_pad = func.pad(seed_img_rot, (pad, pad, pad, pad), 'constant', 0)\n",
    "                tensor = torch.cat([seed_pad, torch.zeros([1, params['_CHANNELS_']-4, _PLAY_SIZE_, _PLAY_SIZE_])], 1).to(_PLAY_DEVICE_)        \n",
    "        # * mouse click events - erase and draw\n",
    "        if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "            mouse_down = True\n",
    "            if pygame.mouse.get_pressed(3)[2] and model_start:\n",
    "                mouse = np.array(pygame.mouse.get_pos(), dtype=float)\n",
    "                pos = mouse / window_size * size\n",
    "                dot = np.zeros_like(tensor.detach().cpu().numpy())\n",
    "                dot[:, 3:, int(pos[1]), int(pos[0])] = 1.0\n",
    "                tensor += torch.tensor(dot).to(_PLAY_DEVICE_)\n",
    "        if event.type == pygame.MOUSEBUTTONUP:\n",
    "            mouse_down = False\n",
    "        if (event.type == pygame.MOUSEMOTION or event.type == pygame.MOUSEBUTTONDOWN) and mouse_down and model_start:\n",
    "            mouse = np.array(pygame.mouse.get_pos(), dtype=float)\n",
    "            pos = mouse / window_size * size\n",
    "            if pygame.mouse.get_pressed(3)[0]:\n",
    "                mask = circle_mask(size, _RADIUS_, pos)\n",
    "                tensor[0:] *= torch.tensor(mask).to(_PLAY_DEVICE_)\n",
    "            \n",
    "    # * update tensor\n",
    "    if model_start:\n",
    "        with torch.no_grad():\n",
    "            tensor = model(tensor)\n",
    "    \n",
    "    # * draw tensor to window\n",
    "    window.fill(_WINDOW_BG_COLOR_)\n",
    "    img = to_rgb(tensor, _alpha=bg_color).squeeze()\n",
    "    vis = (np.array(img.cpu())*255).astype(np.uint8)\n",
    "    if show_vecs:\n",
    "        vecs = tensor[:, -1:].squeeze(0)\n",
    "        vecs = np.array(vecs.cpu())%(2.0*torch.pi)\n",
    "    pixel = pygame.Surface((_WINDOW_SCALE_, _WINDOW_SCALE_))\n",
    "    for j in range(size):\n",
    "        for i in range(size):\n",
    "            color = vis[:, i, j]\n",
    "            # * create vectors for each cell\n",
    "            if show_vecs:\n",
    "                vec_dir = vecs[:, i, j]\n",
    "                vec = imutils.rotate(vec_img, angle=np.rad2deg(vec_dir).item())\n",
    "                vec *= color\n",
    "                surf = pygame.surfarray.make_surface(vec)\n",
    "                pixel.blit(surf, (0, 0))\n",
    "            # * fill cell with color\n",
    "            else:\n",
    "                pixel.fill(color)\n",
    "            draw_me = pygame.Rect(j*_WINDOW_SCALE_, i*_WINDOW_SCALE_, _WINDOW_SCALE_, _WINDOW_SCALE_)\n",
    "            window.blit(pixel, draw_me)\n",
    "    \n",
    "    # * calculate fps\n",
    "    now = datetime.datetime.now()\n",
    "    if (now - prev_time).seconds >= 1.0:\n",
    "        prev_time = now\n",
    "        fps_surface = my_font.render('fps: ' + str(int(fps)), False, _WINDOW_TEXT_COLOR_)\n",
    "        fps = 0\n",
    "    else:\n",
    "        fps += 1       \n",
    "    \n",
    "    # * render text\n",
    "    window.blit(model_surface, (0, 0))\n",
    "    window.blit(seed_surface, (0, font_size))\n",
    "    window.blit(text_surface, (0, window_size-font_size))\n",
    "    window.blit(vec_surface, (0, window_size-font_size*2))\n",
    "    window.blit(fps_surface, (0, window_size-font_size*3))\n",
    "    \n",
    "    # * flip it!\n",
    "    pygame.display.flip()\n",
    "\n",
    "# * quit it!\n",
    "pygame.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
