{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available?  True\n",
      "device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as func\n",
    "import PIL.Image\n",
    "import random\n",
    "import json\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import matplotlib.pylab as pl\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def load_image_as_tensor(_path, _size, _resample=PIL.Image.Resampling.BICUBIC):\n",
    "    img = PIL.Image.open(_path)\n",
    "    img = img.resize((_size, _size), _resample)\n",
    "    img = np.float32(img) / 255.0\n",
    "    img[..., :3] *= img[..., 3:]\n",
    "    return torch.from_numpy(img)[None, ...]\n",
    "\n",
    "def show_tensor_as_image(_img):\n",
    "    img = to_rgb(_img).squeeze()\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    \n",
    "def to_rgb(_x, _alpha='BLACK'):\n",
    "    rgb, a = _x[:, :, :, :3], _x[:, :, :, 3:4]\n",
    "    if _alpha == 'BLACK':\n",
    "        return rgb\n",
    "    elif _alpha == 'WHITE':\n",
    "        return 1.-a + rgb\n",
    "\n",
    "def circle_mask(_size, _radius, _pos):\n",
    "    Y, X = np.ogrid[:_size, :_size]\n",
    "    dist_from_center = np.sqrt((X - _pos[0])**2 + (Y-_pos[1])**2)\n",
    "    mask = dist_from_center >= _radius\n",
    "    return mask\n",
    "\n",
    "def show_batch(batch_size, before, after, dpi=256):\n",
    "    fig = plt.figure(figsize=(batch_size, 2), dpi=dpi)\n",
    "    axarr = fig.subplots(nrows=2, ncols=batch_size)\n",
    "    gspec = gridspec.GridSpec(2, batch_size)\n",
    "    gspec.update(wspace=0.1, hspace=0) # set the spacing between axes.\n",
    "    plt.clf()\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        img_i = before[i]\n",
    "        img_i = img_i[:4].unsqueeze(0)\n",
    "        img_rgb = to_rgb(img_i)\n",
    "        axarr[0, i] = plt.subplot(gspec[i])\n",
    "        axarr[0, i].set_xticks([])\n",
    "        axarr[0, i].set_yticks([])\n",
    "        axarr[0, i].imshow(img_rgb, aspect='equal')\n",
    "        axarr[0, i].set_title(str(i), fontsize=8)\n",
    "        \n",
    "    for i in range(batch_size):\n",
    "        img_i = after[i]\n",
    "        img_i = img_i[:4].unsqueeze(0)\n",
    "        img_rgb = to_rgb(img_i)\n",
    "        axarr[1, i] = plt.subplot(gspec[i+batch_size])\n",
    "        axarr[1, i].set_xticks([])\n",
    "        axarr[1, i].set_yticks([])\n",
    "        axarr[1, i].imshow(img_rgb, aspect='equal')\n",
    "        \n",
    "    plt.show()\n",
    "\n",
    "# * set device\n",
    "_DEVICE_ = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print ('cuda available? ', torch.cuda.is_available())\n",
    "print ('device: ', _DEVICE_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Pre-Made Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed_img.shape:  torch.Size([1, 64, 64, 4])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAFBklEQVR4nO3aMQ4CMQwAwQTd/78cGlgooL2kmClduVtZ8lxrrQEAY4zH7gUAOIcoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoAJBr9wJwjDl/z9e6dw/YyKUAQEQBgIgCABEFACIKAMT3Ebz5MgKXAgAfogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAMi1ewE4xfwzX7duAXu5FACIKAAQUQAgogBARAGA+D6CF19G4FIA4IsoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgDwBx1MNC5BIWpgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_SEED_FILE_ = '_2_seeds_64.png'\n",
    "_SIZE_ = 64\n",
    "\n",
    "seed_img = load_image_as_tensor('..\\\\_seeds\\\\'+_SEED_FILE_,  _SIZE_)\n",
    "print ('seed_img.shape: ', seed_img.shape)\n",
    "show_tensor_as_image(to_rgb(seed_img, 'WHITE'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target Image to Train Model\n",
    "- NOTE: seed _SIZE_ should equal _TARGET-SIZE_ + (2 * _PAD_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target.shape:  torch.Size([1, 64, 64, 4])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_TARGET_FILE_ = 'cowboy.png'\n",
    "_TARGET_SIZE_ = 40\n",
    "_PAD_ = 12\n",
    "\n",
    "target_img = load_image_as_tensor('..\\\\_images\\\\'+_TARGET_FILE_, _TARGET_SIZE_)\n",
    "target_img = target_img.permute(0, 3, 1, 2)\n",
    "target_img = torch.nn.functional.pad(target_img, (_PAD_, _PAD_, _PAD_, _PAD_), 'constant', 0)\n",
    "target_img = target_img.permute(0, 2, 3, 1)\n",
    "print ('target.shape: ', target_img.shape)\n",
    "show_tensor_as_image(target_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptions:\n",
    "- LAPLACIAN: isotropic nca model\n",
    "- SOBEL_MAG: isotrpic nca variant which adds upon the 'laplacian' model by making use of the magnitude of the two directional sobel filters\n",
    "- STEERABLE: angle-based steerable nca\n",
    "- GRADIENT: gradient-based steerable nca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernels\n",
    "ID_KERN = torch.tensor([\n",
    "    [0., 0., 0.], \n",
    "    [0., 1., 0.], \n",
    "    [0., 0., 0.]])\n",
    "SOBEL_KERN = torch.tensor([\n",
    "    [-1., 0., 1.], \n",
    "    [-2., 1., 2.], \n",
    "    [-1., 0., 1.]])\n",
    "LAP_KERN = torch.tensor([\n",
    "    [1.,   2., 1.], \n",
    "    [2., -12., 2.], \n",
    "    [1.,   2., 1.]])\n",
    "\n",
    "def per_channel_conv(_x, _filters):\n",
    "    batch_size, height, width, channels = _x.shape\n",
    "    print ('per-channel conv start')\n",
    "    print ('batch_size: ', batch_size)\n",
    "    print ('height: ', height)\n",
    "    print ('width: ', width)\n",
    "    print ('channels: ', channels)\n",
    "    print ('_x.shape: ', _x.shape)\n",
    "    # * reshape x to make per-channel convolution possible + pad 1 on each side\n",
    "    y = _x.permute(0, 3, 1, 2)\n",
    "    print ('permute y.shape: ', y.shape)\n",
    "    y = y.reshape(batch_size*channels, 1, height, width)\n",
    "    print ('reshaped y.shape: ', y.shape)\n",
    "    y = func.pad(y, (1, 1, 1, 1), 'circular')\n",
    "    print ('padded y.shape: ', y.shape)\n",
    "    # * perform per-channel convolutions\n",
    "    _filters = _filters.to(_DEVICE_)\n",
    "    y = y.to(_DEVICE_)\n",
    "    y = func.conv2d(y, _filters[:, None])\n",
    "    print ('convolved y.shape: ', y.shape)\n",
    "    y = y.reshape(batch_size, -1, height, width)\n",
    "    print ('reshaped y.shape: ', y.shape)\n",
    "    y = y.permute(0, 2, 3, 1)\n",
    "    print ('permute y.shape: ', y.shape)\n",
    "    return y\n",
    "\n",
    "def laplacian_perception(_x):\n",
    "    lap_conv = per_channel_conv(_x, LAP_KERN[None, :])\n",
    "    y = torch.cat([_x, lap_conv], 1)\n",
    "    return y\n",
    "\n",
    "def sobel_mag_perception(_x):\n",
    "    lap_conv = per_channel_conv(_x, LAP_KERN[None, :])\n",
    "    sobel_conv = per_channel_conv(_x, torch.stack([SOBEL_KERN, SOBEL_KERN.T]))\n",
    "    gx, gy = sobel_conv[:, ::2], sobel_conv[:, 1::2]\n",
    "    y = torch.cat([_x, lap_conv, (gx*gx+gy*gy+1e-8).sqrt()], 1)\n",
    "    return y\n",
    "\n",
    "def steerable_perception(_x):\n",
    "    # * separate states and angle channels\n",
    "    states, angle = _x[:, :-1], _x[:, -1:]\n",
    "    filters = torch.stack([SOBEL_KERN, SOBEL_KERN.T])\n",
    "    grad = per_channel_conv(states, filters)\n",
    "    gx, gy = grad[:, ::2], grad[:, 1::2]\n",
    "    # * get cos and sin of angle channel and apply rotation to gx, gy\n",
    "    c, s = angle.cos(), angle.sin()\n",
    "    rot_grad = torch.cat([gx*c+gy*s, gy*c-gx*s], 1)\n",
    "    lap_conv = per_channel_conv(_x, LAP_KERN[None, :])\n",
    "    y = torch.cat([states, rot_grad, lap_conv], 1)\n",
    "\n",
    "def gradient_perception(_x):\n",
    "    filters = torch.stack([SOBEL_KERN, SOBEL_KERN.T])\n",
    "    grad = per_channel_conv(_x, filters)\n",
    "    grad, dir = grad[:, :-2], grad[:, -2:]\n",
    "    dir = dir / dir.norm(dim=1, keepdim=True).clip(1.0)\n",
    "    gx, gy = grad[:, ::2], grad[:, 1::2]\n",
    "    c, s = dir[:, :1], dir[:, 1::2]\n",
    "    rot_grad = torch.cat([gx*c+gy*s, gy*c-gx*s], 1)\n",
    "    lap_conv = per_channel_conv(_x, LAP_KERN[None, :])\n",
    "    y = torch.cat([_x, rot_grad, lap_conv], 1)\n",
    "    return y\n",
    "    \n",
    "perception = {\n",
    "    'LAPLACIAN': laplacian_perception,\n",
    "    'SOBEL_MAG': sobel_mag_perception,\n",
    "    'STEERABLE': steerable_perception,\n",
    "    'GRADIENT': gradient_perception    \n",
    "}\n",
    "\n",
    "def get_alive_mask(_x):\n",
    "    is_alive = (_x[:, :, :, 3:4] > 0.1).to(torch.float32)\n",
    "    y = per_channel_conv(is_alive, LAP_KERN[None, :]) > 0.5\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Cellular Automata Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_CHANNELS_ = 16\n",
    "_MODEL_TYPE_ = 'LAPLACIAN'  #['LAPLACIAN', 'SOBEL_MAG', 'STEERABLE', 'GRADIENT']\n",
    "_STOCHASTIC_UPDATE_RATE_ = 0.5\n",
    "_ANGLE_CHANNEL_ = 1 if _MODEL_TYPE_ == 'STEERABLE' else 0\n",
    "_SCALAR_CHANNEL_ = _CHANNELS_ - _ANGLE_CHANNEL_\n",
    "\n",
    "class NCA(torch.nn.Module):\n",
    "    def __init__(self, _channels=_CHANNELS_, _hidden=128):\n",
    "        super().__init__()\n",
    "        self.channels = _channels\n",
    "        \n",
    "        # * determine number of perceived channels\n",
    "        perception_channels = perception[_MODEL_TYPE_](torch.zeros([1, _SIZE_, _SIZE_, _channels]).to(_DEVICE_)).shape[3]\n",
    "        \n",
    "        # * determine hidden channels (equalize the parameter count btwn model types)\n",
    "        hidden_channels = 8*1024 // (perception_channels+_channels)\n",
    "        hidden_channels = (_hidden+31) // 32*32\n",
    "        \n",
    "        # * model layers\n",
    "        self.conv1 = torch.nn.Conv2d(perception_channels, hidden_channels, 1)\n",
    "        self.conv2 = torch.nn.Conv2d(hidden_channels, _channels, 1, bias=False)\n",
    "        self.conv2.weight.data.zero_()\n",
    "        \n",
    "    def forward(self, _x):\n",
    "        print ('forward start')\n",
    "        print ('init _x.shape: ', _x.shape)\n",
    "        # * get alive mask\n",
    "        alive_mask = get_alive_mask(_x)\n",
    "        \n",
    "        # * perception step\n",
    "        _x = _x.to(_DEVICE_)\n",
    "        print ('before perception _x.shape: ', _x.shape)\n",
    "        y = perception[_MODEL_TYPE_](_x)\n",
    "        print ('after perception y.shape: ', y.shape)\n",
    "        \n",
    "        # * update step\n",
    "        y = y.permute(0, 3, 1, 2)\n",
    "        y = self.conv2(torch.relu(self.conv1(y)))\n",
    "        y = y.permute(0, 2, 3, 1)\n",
    "        \n",
    "        # * create stochastic update mask\n",
    "        batch_size, height, width, channels = _x.shape\n",
    "        stochastic_update_mask = (torch.rand(batch_size, height, width, 1) + _STOCHASTIC_UPDATE_RATE_).floor()\n",
    "        \n",
    "        # * perform update\n",
    "        x = _x + y * stochastic_update_mask\n",
    "        if _SCALAR_CHANNEL_ == _CHANNELS_:\n",
    "            x = x * alive_mask\n",
    "        else:\n",
    "            states = x[:, :_SCALAR_CHANNEL_] * alive_mask\n",
    "            angle = x[:, _SCALAR_CHANNEL_:] % (2.0*torch.pi)\n",
    "            x = torch.cat([states, angle], 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Functions:\n",
    "- PIXEL-WISE: L2-norm loss\n",
    "- INVARIANT: rotation-invariant loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_LOSS_FUNC_ = 'PIXEL_WISE' #['PIXEL_WISE', 'INVARIANT']\n",
    "_LOWER_LR_ = 1e-5\n",
    "_UPPER_LR_ = 1e-3\n",
    "\n",
    "def pixel_wise_loss_func(_x, _target, _scale=1e3):\n",
    "    return _scale * ((_target[:, :, :, :4, ...] - _x[:, :, :, :4, ...]) ** 2).mean(dim=[1, 2, 3])\n",
    "\n",
    "def invariant_loss_func(_x):\n",
    "    raise NotImplementedError\n",
    "\n",
    "loss_func = {\n",
    "    'PIXEL_WISE': pixel_wise_loss_func,\n",
    "    'INVARIANT': invariant_loss_func\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per-channel conv start\n",
      "batch_size:  1\n",
      "height:  64\n",
      "width:  64\n",
      "channels:  16\n",
      "_x.shape:  torch.Size([1, 64, 64, 16])\n",
      "permute y.shape:  torch.Size([1, 16, 64, 64])\n",
      "reshaped y.shape:  torch.Size([16, 1, 64, 64])\n",
      "padded y.shape:  torch.Size([16, 1, 66, 66])\n",
      "convolved y.shape:  torch.Size([16, 1, 64, 64])\n",
      "reshaped y.shape:  torch.Size([1, 16, 64, 64])\n",
      "permute y.shape:  torch.Size([1, 64, 64, 16])\n",
      "forward start\n",
      "init _x.shape:  torch.Size([8, 64, 64, 16])\n",
      "per-channel conv start\n",
      "batch_size:  8\n",
      "height:  64\n",
      "width:  64\n",
      "channels:  1\n",
      "_x.shape:  torch.Size([8, 64, 64, 1])\n",
      "permute y.shape:  torch.Size([8, 1, 64, 64])\n",
      "reshaped y.shape:  torch.Size([8, 1, 64, 64])\n",
      "padded y.shape:  torch.Size([8, 1, 66, 66])\n",
      "convolved y.shape:  torch.Size([8, 1, 64, 64])\n",
      "reshaped y.shape:  torch.Size([8, 1, 64, 64])\n",
      "permute y.shape:  torch.Size([8, 64, 64, 1])\n",
      "before perception _x.shape:  torch.Size([8, 64, 64, 16])\n",
      "per-channel conv start\n",
      "batch_size:  8\n",
      "height:  64\n",
      "width:  64\n",
      "channels:  16\n",
      "_x.shape:  torch.Size([8, 64, 64, 16])\n",
      "permute y.shape:  torch.Size([8, 16, 64, 64])\n",
      "reshaped y.shape:  torch.Size([128, 1, 64, 64])\n",
      "padded y.shape:  torch.Size([128, 1, 66, 66])\n",
      "convolved y.shape:  torch.Size([128, 1, 64, 64])\n",
      "reshaped y.shape:  torch.Size([8, 16, 64, 64])\n",
      "permute y.shape:  torch.Size([8, 64, 64, 16])\n",
      "after perception y.shape:  torch.Size([8, 128, 64, 16])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (128) must match the size of tensor b (64) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Marco\\Documents\\GitHub\\neural-cellular-automata\\1_isotropic_nca\\isotropic_nca.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Marco/Documents/GitHub/neural-cellular-automata/1_isotropic_nca/isotropic_nca.ipynb#X20sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_steps):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Marco/Documents/GitHub/neural-cellular-automata/1_isotropic_nca/isotropic_nca.ipynb#X20sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m     prev_x \u001b[39m=\u001b[39m x\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Marco/Documents/GitHub/neural-cellular-automata/1_isotropic_nca/isotropic_nca.ipynb#X20sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m     x \u001b[39m=\u001b[39m model(x)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Marco/Documents/GitHub/neural-cellular-automata/1_isotropic_nca/isotropic_nca.ipynb#X20sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m     diff_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (x \u001b[39m-\u001b[39m prev_x)\u001b[39m.\u001b[39mabs()\u001b[39m.\u001b[39mmean()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Marco/Documents/GitHub/neural-cellular-automata/1_isotropic_nca/isotropic_nca.ipynb#X20sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m     overflow_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (x \u001b[39m-\u001b[39m x\u001b[39m.\u001b[39mclamp(\u001b[39m-\u001b[39m\u001b[39m2.0\u001b[39m, \u001b[39m2.0\u001b[39m))[:, :, :, :_SCALAR_CHANNEL_]\u001b[39m.\u001b[39msquare()\u001b[39m.\u001b[39msum()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\Marco\\Documents\\GitHub\\neural-cellular-automata\\1_isotropic_nca\\isotropic_nca.ipynb Cell 14\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Marco/Documents/GitHub/neural-cellular-automata/1_isotropic_nca/isotropic_nca.ipynb#X20sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m stochastic_update_mask \u001b[39m=\u001b[39m (torch\u001b[39m.\u001b[39mrand(batch_size, height, width, \u001b[39m1\u001b[39m) \u001b[39m+\u001b[39m _STOCHASTIC_UPDATE_RATE_)\u001b[39m.\u001b[39mfloor()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Marco/Documents/GitHub/neural-cellular-automata/1_isotropic_nca/isotropic_nca.ipynb#X20sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# * perform update\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Marco/Documents/GitHub/neural-cellular-automata/1_isotropic_nca/isotropic_nca.ipynb#X20sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m x \u001b[39m=\u001b[39m _x \u001b[39m+\u001b[39m y \u001b[39m*\u001b[39;49m stochastic_update_mask\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Marco/Documents/GitHub/neural-cellular-automata/1_isotropic_nca/isotropic_nca.ipynb#X20sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mif\u001b[39;00m _SCALAR_CHANNEL_ \u001b[39m==\u001b[39m _CHANNELS_:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Marco/Documents/GitHub/neural-cellular-automata/1_isotropic_nca/isotropic_nca.ipynb#X20sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m*\u001b[39m alive_mask\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (128) must match the size of tensor b (64) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "_NAME_ = 'cowboy'\n",
    "_EPOCHS_ = 15000\n",
    "_POOL_SIZE_ = 256\n",
    "_BATCH_SIZE_ = 8\n",
    "_NUM_DAMG_ = 4\n",
    "\n",
    "_DAMG_RATE_ = 5\n",
    "_INFO_RATE_ = 20\n",
    "_SAVE_RATE_ = 500\n",
    "\n",
    "# * save model method\n",
    "def save_model(_dir, _model, _name):\n",
    "    model_path = pathlib.Path(_dir)\n",
    "    model_path.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save(_model.state_dict(), _dir + '\\\\' + _name + '.pt')\n",
    "    \n",
    "    # * save model parameters\n",
    "    dict = {\n",
    "        '_SEED_FILE_': _SEED_FILE_,\n",
    "        '_SIZE_': _SIZE_,\n",
    "        \n",
    "        '_TARGET_FILE_': _TARGET_FILE_,\n",
    "        '_TARGET_SIZE_': _TARGET_SIZE_,\n",
    "        '_PAD_': _PAD_,\n",
    "        \n",
    "        '_CHANNELS_': _CHANNELS_,\n",
    "        '_MODEL_TYPE_': _MODEL_TYPE_,\n",
    "        '_STOCHASTIC_UPDATE_RATE_': _STOCHASTIC_UPDATE_RATE_,\n",
    "        '_ANGLE_CHANNEL_': _ANGLE_CHANNEL_,\n",
    "        '_SCALAR_CHANNEL_': _SCALAR_CHANNEL_,\n",
    "        \n",
    "        '_LOSS_FUNC_': _LOSS_FUNC_,\n",
    "        '_LOWER_LR_': _LOWER_LR_,\n",
    "        '_UPPER_LR_': _UPPER_LR_,\n",
    "        \n",
    "        '_NAME_': _NAME_,\n",
    "        '_EPOCHS_': _EPOCHS_,\n",
    "        '_POOL_SIZE_': _POOL_SIZE_,\n",
    "        '_BATCH_SIZE_': _BATCH_SIZE_,\n",
    "        '_NUM_DAMG_': _NUM_DAMG_,\n",
    "        \n",
    "        '_DAMG_RATE_': _DAMG_RATE_,\n",
    "        '_INFO_RATE_': _INFO_RATE_,\n",
    "        '_SAVE_RATE_': _SAVE_RATE_,\n",
    "        '_DEVICE_': _DEVICE_,\n",
    "    }\n",
    "    json_object = json.dumps(dict, indent=4)\n",
    "    with open(_dir + '\\\\' + _name + '_params.json', 'w') as outfile:\n",
    "        outfile.write(json_object)\n",
    "        print ('model saved!')\n",
    "\n",
    "# * create model\n",
    "model = NCA().to(_DEVICE_)\n",
    "opt = torch.optim.Adam(model.parameters(), _UPPER_LR_)\n",
    "lr_sched = torch.optim.lr_scheduler.CyclicLR(opt, _LOWER_LR_, _UPPER_LR_, step_size_up=2000, mode='triangular2', cycle_momentum=False)\n",
    "\n",
    "# * create seed pool\n",
    "target = torch.cat([target_img, torch.zeros([1, _SIZE_, _SIZE_, _CHANNELS_-4])], 3).to(_DEVICE_)\n",
    "seed = torch.cat([seed_img, torch.zeros([1, _SIZE_, _SIZE_, _CHANNELS_-4])], 3).to(_DEVICE_)\n",
    "pool = seed.clone().repeat(_POOL_SIZE_, 1, 1, 1)\n",
    "\n",
    "loss_log = []\n",
    "progress = 0\n",
    "\n",
    "# * begin training \n",
    "for _ in range(_EPOCHS_+1):\n",
    "    with torch.no_grad():\n",
    "        # * sample batch from pool\n",
    "        i = len(loss_log)\n",
    "        batch_idxs = np.random.choice(_POOL_SIZE_, _BATCH_SIZE_, replace=False)\n",
    "        x = pool[batch_idxs]\n",
    "        \n",
    "        # * re-order batch based on loss\n",
    "        loss_ranks = torch.argsort(loss_func[_LOSS_FUNC_](x, target), descending=True)\n",
    "        x = x[loss_ranks]\n",
    "        \n",
    "        # * re-add seed into batch\n",
    "        seed_swap_rate = 1 if i < 4000 else 5\n",
    "        if i % seed_swap_rate == 0:\n",
    "            x[:1] = seed\n",
    "            \n",
    "        # * damage lowest loss in batch\n",
    "        if i > 4000 and i % _DAMG_RATE_ == 0:\n",
    "            radius = random.uniform(_SIZE_*0.1, _SIZE_*0.3)\n",
    "            u = random.uniform(0, 1) * _SIZE_\n",
    "            v = random.uniform(0, 1) * _SIZE_\n",
    "            mask = circle_mask(_SIZE_, radius, [u, v])\n",
    "            x[-_NUM_DAMG_:] *= torch.tensor(mask).to(_DEVICE_)\n",
    "            \n",
    "    # * calculate loss\n",
    "    overflow_loss = 0.0\n",
    "    diff_loss = 0.0\n",
    "    target_loss = 0.0\n",
    "    num_steps = np.random.randint(64, 96)\n",
    "    \n",
    "    # visualize batch\n",
    "    if i % _INFO_RATE_ == 0:\n",
    "        before = x.detach().cpu()\n",
    "    \n",
    "    # * forward pass\n",
    "    for _ in range(num_steps):\n",
    "        prev_x = x\n",
    "        x = model(x)\n",
    "        diff_loss += (x - prev_x).abs().mean()\n",
    "        overflow_loss += (x - x.clamp(-2.0, 2.0))[:, :, :, :_SCALAR_CHANNEL_].square().sum()\n",
    "        \n",
    "    target_loss += loss_func[_LOSS_FUNC_](x, target)\n",
    "    diff_loss *= 10.0\n",
    "    loss = target_loss + overflow_loss + diff_loss\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        loss.backward()\n",
    "        for p in model.parameters():\n",
    "            p.grad /= (p.grad.norm()+1e-8)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        lr_sched.step()\n",
    "        pool[batch_idxs] = x\n",
    "        loss_log.append(loss.item())\n",
    "        \n",
    "        # * print out info\n",
    "        if i % _INFO_RATE_ == 0:\n",
    "            # * show loss plot\n",
    "            clear_output(True)\n",
    "            pl.plot(loss_log, '.', alpha=0.1)\n",
    "            pl.yscale('log')\n",
    "            pl.ylim(np.min(loss_log), loss_log[0])\n",
    "            pl.show()\n",
    "            \n",
    "            # * show batch\n",
    "            after = x.detach().cpu()\n",
    "            show_batch(_BATCH_SIZE_, before, after)\n",
    "            \n",
    "            # * print info\n",
    "            print('\\rstep:', i, '\\tloss:', loss.item(), '\\tlr:', lr_sched.get_lr()[0], end='')\n",
    "            \n",
    "        # * save checkpoint\n",
    "        if i % _SAVE_RATE_ == 0:\n",
    "            save_model('checkpoints', model, _NAME_+'_cp'+str(i))\n",
    "            \n",
    "# * save final model\n",
    "save_model('models', model, _NAME_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
