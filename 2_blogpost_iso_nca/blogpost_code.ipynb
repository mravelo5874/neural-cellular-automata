{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "974P6JcnyfPf"
      },
      "source": [
        "# Isotropic and Steerable NCA (structured seed experiments)\n",
        "\n",
        "### Author: Craig Fouts (cwf2117@columbia.edu)\n",
        "\n",
        "*Copyright 2023 Craig Fouts*\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "[https://www.apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "NCTKsrsiOUiG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU 0: NVIDIA GeForce GTX 1660 Ti (UUID: GPU-d77bcb98-b41a-4494-1c93-f4f7d0b55d9c)\n",
            "cuda available?  True\n",
            "device:  cuda\n"
          ]
        }
      ],
      "source": [
        "#@title Notebook Utilities and Setup\n",
        "\n",
        "import base64\n",
        "import io\n",
        "import matplotlib.pylab as pl\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL.Image, PIL.ImageDraw, PIL.ImageFont\n",
        "import requests\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms.functional as T\n",
        "import warnings\n",
        "from colorsys import hsv_to_rgb\n",
        "# from google.colab import drive, output\n",
        "from IPython.display import clear_output, Image\n",
        "from torch.nn import BatchNorm1d, Dropout, InstanceNorm1d, LayerNorm, Module, ReLU, Sequential\n",
        "from torchvision.transforms.functional_tensor import gaussian_blur\n",
        "from tqdm.notebook import tnrange\n",
        "from tqdm import tqdm\n",
        "\n",
        "os.environ['FFMPEG_BINARY'] = 'ffmpeg'\n",
        "import moviepy.editor as mvp\n",
        "from moviepy.video.io.ffmpeg_writer import FFMPEG_VideoWriter\n",
        "\n",
        "# output.enable_custom_widget_manager()\n",
        "\n",
        "# * find GPU available\n",
        "clear_output()\n",
        "!nvidia-smi -L\n",
        "\n",
        "# * sets the device\n",
        "# *     defaults to 'cuda'\n",
        "_DEVICE_ = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print ('cuda available? ', torch.cuda.is_available())\n",
        "print ('device: ', _DEVICE_)\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.cuda.empty_cache()\n",
        "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# USE_DRIVE = False  #@param{type:\"boolean\"}\n",
        "DIRECTORY = 'My Drive/Models'  #@param{type:\"string\"}\n",
        "MODEL_PATH = '_checkpoints'\n",
        "\n",
        "# if USE_DRIVE:\n",
        "#   drive.mount('/content/drive')\n",
        "#   MODEL_PATH = os.path.join('/content/drive', DIRECTORY)\n",
        "\n",
        "def imread(url, max_size=None, mode=None):\n",
        "  if url.startswith(('http:', 'https:')):\n",
        "    headers = {'User-Agent': 'Requests in Colab/0.0 (https://colab.research.google.com/; no-reply@google.com) requests/0.0'}\n",
        "    r = requests.get(url, headers=headers)\n",
        "    f = io.BytesIO(r.content)\n",
        "  else:\n",
        "    f = url\n",
        "  img = PIL.Image.open(f)\n",
        "  if max_size is not None:\n",
        "    img.thumbnail((max_size, max_size), PIL.Image.ANTIALIAS)\n",
        "  if mode is not None:\n",
        "    img = img.convert(mode)\n",
        "  img = np.float32(img) / 255.\n",
        "  return img\n",
        "\n",
        "def np2pil(a):\n",
        "  if a.dtype in (np.float32, np.float64):\n",
        "    a = np.uint8(np.clip(a, 0, 1)*255)\n",
        "  return PIL.Image.fromarray(a)\n",
        "\n",
        "def imwrite(f, a, fmt=None, quality=95):\n",
        "  a = np.asarray(a)\n",
        "  if isinstance(f, str):\n",
        "    fmt = f.rsplit('.', 1)[-1].lower()\n",
        "    if fmt == 'jpg':\n",
        "      fmt = 'jpeg'\n",
        "    f = open(f, 'wb')\n",
        "  np2pil(a).save(f, fmt, quality=quality)\n",
        "\n",
        "def imencode(a, fmt='jpeg'):\n",
        "  a = np.asarray(a)\n",
        "  if a.ndim == 3 and a.shape[-1] == 4:\n",
        "    fmt = 'png'\n",
        "  f = io.BytesIO()\n",
        "  imwrite(f, a, fmt)\n",
        "  return f.getvalue()\n",
        "\n",
        "def im2url(a, fmt='jpeg'):\n",
        "  encoded = imencode(a, fmt)\n",
        "  base64_byte_string = base64.b64encode(encoded).decode('ascii')\n",
        "  return 'data:image/' + fmt.upper() + ';base64,' + base64_byte_string\n",
        "\n",
        "def zoom(img, scale=4):\n",
        "  img = np.repeat(img, scale, 0)\n",
        "  img = np.repeat(img, scale, 1)\n",
        "  return img\n",
        "\n",
        "def imshow(a, fmt='jpeg', scale=4):\n",
        "  display(Image(data=imencode(zoom(a, scale), fmt)))\n",
        "\n",
        "def tile2d(a, w=None):\n",
        "  a = np.asarray(a)\n",
        "  if w is None:\n",
        "    w = int(np.ceil(np.sqrt(a.shape[0])))\n",
        "  th, tw = a.shape[1:3]\n",
        "  pad = (w - a.shape[0]) % w\n",
        "  a = np.pad(a, [(0, pad)]+[(0, 0)]*(a.ndim-1), 'constant')\n",
        "  h = a.shape[0] // w\n",
        "  a = a.reshape([h, w]+list(a.shape[1:]))\n",
        "  a = np.rollaxis(a, 2, 1).reshape([th*h, tw*w]+list(a.shape[4:]))\n",
        "  return a\n",
        "\n",
        "def to_rgb(x):\n",
        "  rgb, a = x[:, :3], x[:, 3:4]\n",
        "  return 1. - a + rgb\n",
        "\n",
        "def grab_plot(close=True):\n",
        "  fig = pl.gcf()\n",
        "  fig.canvas.draw()\n",
        "  img = np.array(fig.canvas.renderer._renderer)\n",
        "  a = np.float32(img[..., 3:]/255.)\n",
        "  img = np.uint8(255*(1.-a)+img[..., :3]*a)\n",
        "  if close:\n",
        "    pl.close()\n",
        "  return img\n",
        "\n",
        "def vis_angle(x, w):\n",
        "  m = get_alive_mask(x).cpu()\n",
        "  rgb = to_rgb(x)[0].clip(0, 1).permute(1, 2, 0).cpu()\n",
        "  ang, a, m = x[0, -1].cpu(), x[0, 3].cpu(), m[0,0]\n",
        "  c, s = ang.cos() * a, ang.sin() * a\n",
        "  px, py = np.mgrid[-1:1:w*1j, -1:1:w*1j]\n",
        "  pl.figure(figsize=(10, 10))\n",
        "  pl.axis('equal')\n",
        "  pl.xlim(-0.8, 0.8); pl.ylim(-0.8,0.8)\n",
        "  pl.quiver(px[m], py[m], c[m], s[m], color=rgb[m], pivot='mid', scale_units='xy', units='xy', scale=W/3)\n",
        "  pl.tight_layout()\n",
        "  pl.axis('off')\n",
        "  return grab_plot()\n",
        "\n",
        "class VideoWriter:\n",
        "  def __init__(self, filename='_autoplay.mp4', fps=30., **kwargs):\n",
        "    self.writer = None\n",
        "    self.params = dict(filename=filename, fps=fps, **kwargs)\n",
        "\n",
        "  def __enter__(self):\n",
        "    return self\n",
        "  \n",
        "  def __exit__(self, *args):\n",
        "    self.close()\n",
        "    if self.params['filename'] == '_autoplay.mp4':\n",
        "      self.show()\n",
        "\n",
        "  def add(self, img):\n",
        "    img = np.asarray(img)\n",
        "    if self.writer is None:\n",
        "      h, w = img.shape[:2]\n",
        "      self.writer = FFMPEG_VideoWriter(size=(w, h), **self.params)\n",
        "    if img.dtype in (np.float32, np.float64):\n",
        "      img = np.uint8(img.clip(0, 1)*255)\n",
        "    if img.ndim == 2:\n",
        "      img = np.repeat(img[..., None], 3, -1)\n",
        "    self.writer.write_frame(img)\n",
        "\n",
        "  def close(self):\n",
        "    if self.writer is not None:\n",
        "      self.writer.close()\n",
        "\n",
        "  def show(self, **kwargs):\n",
        "    self.close()\n",
        "    fn = self.params['filename']\n",
        "    display(mvp.ipython_display(fn, **kwargs))\n",
        "\n",
        "def vidgen(filename, model, n_frames=500, max_speed=128, n=16, sz=72, p=2, r=4, angle=None, custom_seed=None):\n",
        "  with VideoWriter(filename=filename) as vid, torch.no_grad():\n",
        "    if custom_seed is None:\n",
        "        x = seed(n, sz, p, r, angle=angle)\n",
        "    for i in tnrange(n_frames, leave=False):\n",
        "        img = to_rgb(x).permute(0, 2, 3, 1).cpu()\n",
        "        vid.add(zoom(tile2d(img), 2))\n",
        "        step_n = min(2**(i//30), max_speed)\n",
        "        for _ in range(step_n):\n",
        "            x = model(x)\n",
        "    vid.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4azYa9u3aMT"
      },
      "source": [
        "# Model Legend\n",
        "\n",
        "Select the model of interest from the following\n",
        "\n",
        "* LAPLACIAN:&emsp;Isotropic NCA model\n",
        "* LAP6:&emsp;Isotropic NCA, (trained and/or evaluated) on a hexagonal grid\n",
        "* GRADNORM:&emsp;Isotropic NCA variant discussed in the blogpost\n",
        "* STEERABLE:&emsp;Angle-based Steerable NCA\n",
        "* GRADIENT:&emsp;Gradient-based Steerable NCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "08RFfJ5xl-cL"
      },
      "outputs": [],
      "source": [
        "#@title Model Utilities and Setup {vertical-output:true}\n",
        "\n",
        "MODEL_TYPE = 'STEERABLE'  #@param['LAPLACIAN', 'LAP6', 'GRADNORM', 'STEERABLE', 'GRADIENT']\n",
        "UPDATE_RATE = .5\n",
        "\n",
        "CHN = 16\n",
        "ANGLE_CHN = 1 if MODEL_TYPE == 'STEERABLE' else 0\n",
        "SCALAR_CHN = CHN - ANGLE_CHN\n",
        "\n",
        "IDENT = torch.tensor([[0., 0., 0.], [0., 1., 0.], [0., 0., 0.]])\n",
        "SOBEL = torch.tensor([[-1., 0., 1.], [-2., 0., 2.], [-1., 0., 1.]])\n",
        "LAP = torch.tensor([[1., 2., 1.], [2., -12., 2.], [1., 2., 1.]])\n",
        "LAP6 = torch.tensor([[0., 2., 2.], [2., -12., 2.], [2., 2., 0.]])\n",
        "GAUSS = torch.tensor([[1., 2., 1.], [2., 4., 2.], [1., 2., 1.]]) / 16.\n",
        "NHOOD_KERNEL = ((LAP6 if MODEL_TYPE == 'LAP6' else LAP) != 0.).to(torch.float32)\n",
        "\n",
        "def perchannel_conv(x, filters):\n",
        "  b, ch, h, w = x.shape\n",
        "  y = x.reshape(b * ch, 1, h, w)\n",
        "  y = F.pad(y, (1, 1, 1, 1), 'circular')\n",
        "  filters = filters.to(_DEVICE_)\n",
        "  y = y.to(_DEVICE_)\n",
        "  y = F.conv2d(y, filters[:, None])\n",
        "  return y.reshape(b, -1, h, w)\n",
        "\n",
        "# Isotropic models\n",
        "def laplacian_perception(x):\n",
        "  state_lap = perchannel_conv(x, LAP[None, :])\n",
        "  return torch.cat([x, state_lap], 1)\n",
        "\n",
        "def lap6_perception(x):\n",
        "  state_lap = perchannel_conv(x, LAP6[None, :])\n",
        "  return torch.cat([x, state_lap], 1)\n",
        "\n",
        "def gradnorm_perception(x):\n",
        "  grad = perchannel_conv(x, torch.stack([SOBEL, SOBEL.T]))\n",
        "  gx, gy = grad[:, ::2], grad[:, 1::2]\n",
        "  state_lap = perchannel_conv(x, LAP[None, :])\n",
        "  return torch.cat([x, state_lap, (gx*gx+gy*gy+1e-8).sqrt()], 1)\n",
        "\n",
        "# Steerable models\n",
        "def steerable_perception(x):\n",
        "  state, angle = x[:, :-1], x[:, -1:]\n",
        "  c, s = angle.cos(), angle.sin()\n",
        "  filters = torch.stack([SOBEL, SOBEL.T])\n",
        "  grad = perchannel_conv(state, filters)\n",
        "  gx, gy = grad[:, ::2], grad[:, 1::2]\n",
        "  rot_grad = torch.cat([gx*c+gy*s, gy*c-gx*s], 1)\n",
        "  state_lap = perchannel_conv(state, LAP[None, :])\n",
        "  res = torch.cat([state, rot_grad, state_lap], 1)\n",
        "  return res\n",
        "\n",
        "def gradient_perception(x):\n",
        "  filters = torch.stack([SOBEL, SOBEL.T])\n",
        "  grad = perchannel_conv(x, filters)\n",
        "  grad, dir = grad[:, :-2], grad[:, -2:]\n",
        "  dir = dir / dir.norm(dim=1, keepdim=True).clip(1.)\n",
        "  c, s = dir[:, :1], dir[:, 1:2]\n",
        "  gx, gy = grad[:, ::2], grad[:, 1::2]\n",
        "  rot_grad = torch.cat([gx*c+gy*s, gy*c-gx*s], 1)\n",
        "  state_lap = perchannel_conv(x, LAP[None, :])\n",
        "  return torch.cat([x, state_lap, rot_grad], 1)\n",
        "\n",
        "perception = {\n",
        "    'LAPLACIAN': laplacian_perception,\n",
        "    'LAP6': lap6_perception,\n",
        "    'GRADNORM': gradnorm_perception,\n",
        "    'STEERABLE': steerable_perception,\n",
        "    'GRADIENT': gradient_perception\n",
        "}\n",
        "\n",
        "def get_alive_mask(x):\n",
        "  mature = (x[:, 3:4] > .1).to(torch.float32)\n",
        "  return perchannel_conv(mature, NHOOD_KERNEL[None, :]) > .5\n",
        "\n",
        "def fibonacci_lattice(n):\n",
        "  '''Generates an n-point fibonacci lattice of radius 1'''\n",
        "  epsilon = 0.33  # Assumes n < 24\n",
        "  golden_ratio = (1 + np.sqrt(5)) / 2.\n",
        "  pts = torch.arange(n)\n",
        "  theta = 2 * np.pi * pts / golden_ratio\n",
        "  phi = torch.arccos(1-2*(pts+epsilon)/(n-1+2*epsilon))\n",
        "  x, y, z = torch.cos(theta)*torch.sin(phi), torch.sin(theta)*torch.sin(phi), torch.cos(phi)\n",
        "  return torch.concat([x[None, :], y[None, :], z[None, :]], 0)\n",
        "\n",
        "def rgb_linspace(n):\n",
        "  '''Generates n visually distinct rgb combinations'''\n",
        "  return torch.tensor([hsv_to_rgb(i / n, 1.0, 1.0) for i in range(n)], dtype=torch.float32)\n",
        "\n",
        "def rotate_n(x, n, min=0., max=360.):\n",
        "  a = np.linspace(0., 360., n)\n",
        "  for i, a in zip(range(n), a):\n",
        "    x[i] = T.rotate(x[i], a)\n",
        "  return x\n",
        "\n",
        "def seed(n, sz=128, p=2, r=4, xy=None, angle=0., flip=False, rgb_dist=rgb_linspace):\n",
        "  '''Generates a uniform p-point structured seed of radius r'''\n",
        "  x = torch.zeros(n, CHN, sz, sz)\n",
        "  if SCALAR_CHN != CHN:\n",
        "    x[:, -1] = torch.rand(n, sz, sz) * np.pi * 2.\n",
        "  # Initialize p points equidistant around a circle of radius r\n",
        "  t = np.linspace(0, 2 * np.pi, p, endpoint=False)\n",
        "  if xy is None:\n",
        "    xy = (np.c_[r*np.cos(t), r*np.sin(t)]+(sz//2)).astype(np.int32).T\n",
        "  # Assign distinct rgb values to each point\n",
        "  x[:, :3, xy[0], xy[1]] = rgb_dist(xy.shape[1]).T\n",
        "  x[:, 3:SCALAR_CHN, xy[0], xy[1]] = 1.0\n",
        "  x = rotate_n(x, n) if angle is None else T.rotate(x, angle)\n",
        "  if flip:\n",
        "    x = torch.flip(x, [3])\n",
        "  return x\n",
        "\n",
        "class CA(torch.nn.Module):\n",
        "  def __init__(self, chn=CHN, hidden_n=128):\n",
        "    super().__init__()\n",
        "    self.chn = chn\n",
        "\n",
        "    # Determine the number of perceived channels\n",
        "    perc_n = perception[MODEL_TYPE](torch.zeros([1, chn, 8, 8])).shape[1]\n",
        "\n",
        "    # Approximately equalize the parameter count between model variants\n",
        "    hidden_n = 8 * 1024 // (perc_n + chn)\n",
        "    hidden_n = (hidden_n + 31) // 32 * 32\n",
        "\n",
        "    # Model layers\n",
        "    self.w1 = torch.nn.Conv2d(perc_n, hidden_n, 1)\n",
        "    self.w2 = torch.nn.Conv2d(hidden_n, chn, 1, bias=False)\n",
        "    self.w2.weight.data.zero_()\n",
        "\n",
        "  def forward(self, x, update_rate=UPDATE_RATE):\n",
        "    # Get update and masks\n",
        "    alive = get_alive_mask(x)\n",
        "    y = perception[MODEL_TYPE](x)\n",
        "    y = self.w2(torch.relu(self.w1(y)))\n",
        "    b, c, h, w = y.shape\n",
        "    update_mask = (torch.rand(b, 1, h, w) + update_rate).floor()\n",
        "\n",
        "    # Perform update\n",
        "    x = x + y * update_mask\n",
        "    if SCALAR_CHN == CHN:\n",
        "      x = x * alive\n",
        "    else:\n",
        "      state = x[:, :SCALAR_CHN] * alive\n",
        "      angle = x[:, SCALAR_CHN:] % (2. * torch.pi)\n",
        "      x = torch.cat([state, angle], 1)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF4nrfDn5lf4"
      },
      "source": [
        "# Target Legend\n",
        "\n",
        "Select the target of interest from the following\n",
        "\n",
        "* LIZARD:&emsp;ü¶é\n",
        "* HEART:&emsp;‚ù§Ô∏è\n",
        "* SMILEY:&emsp;üòÅ\n",
        "\n",
        "Select the number of seeds and seed radius below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "LPUMYyptdiDx"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAACgCAYAAACLz2ctAAAAp0lEQVR4nO3cMQ6AUAgDUPD+d8aZ8cehGt/b2EjTuVUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABApx/4vJlZd7dMD1zpB/g3BSRKAQEAAAAAAAAAAAAAAAAAAAAAgMWW3UNTtfYBW6ZHrGMRpYBEKSAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAvMwN1/EFD65GJCAAAAAASUVORK5CYII=",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title Seed and Target Setup {vertical-output:true}\n",
        "\n",
        "SEED_N = 2  #@param{type:\"integer\"}\n",
        "SEED_R = 4  #@param{type:\"integer\"}\n",
        "TARGET = 'COWBOY'  #@param['LIZARD', 'HEART', 'SMILEY']\n",
        "PADDING = 12\n",
        "\n",
        "# emoji = {\n",
        "#     'LIZARD':  'ü¶é',\n",
        "#     'HEART':  '‚ù§Ô∏è',\n",
        "#     'SMILEY': 'üòÅ'\n",
        "# }[TARGET][0]\n",
        "# code = hex(ord(emoji))[2:].lower()\n",
        "url = '../_images/cowboy.png' #https://github.com/googlefonts/noto-emoji/blob/main/png/128/emoji_u%s.png?raw=true'%code\n",
        "target = imread(url, 40)\n",
        "\n",
        "# Show lineup\n",
        "n_seed = seed(1, 40, SEED_N, SEED_R)[0, :4].permute([1, 2, 0]).cpu()\n",
        "imgs = np.stack([n_seed, target], 0)\n",
        "imshow(n_seed)\n",
        "imshow(target)\n",
        "\n",
        "# Format target\n",
        "target[:, :, :3] *= target[:, :, 3:]\n",
        "target = F.pad(torch.tensor(target).permute(2, 0, 1), [*(PADDING,)*4, 0, 0])\n",
        "W = target.shape[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fR5RlK-97Kg2"
      },
      "source": [
        "# Loss Legend\n",
        "\n",
        "Select the loss function of interest from the following\n",
        "\n",
        "* FIXED:&emsp;L2-norm loss\n",
        "* INVARIANT:&emsp;Rotation-invariant loss\n",
        "\n",
        "Select the lower and upper learning rate limits below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "id": "QaqCjNgCmmjS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model name:  steerable_cowboy_fixed_2pt_4r\n"
          ]
        }
      ],
      "source": [
        "#@title Training Utilities and Setup\n",
        "\n",
        "LOSS_FN = 'FIXED'  #@param['FIXED', 'INVARIANT']\n",
        "LOWER_LR = 1e-5  #@param{type:\"number\"}\n",
        "UPPER_LR = 1e-3  #@param{type:\"number\"}\n",
        "MODEL_NAME = f'{MODEL_TYPE}_{TARGET}_{LOSS_FN}_{SEED_N}pt_{SEED_R}r'.lower()\n",
        "MODEL_DIR = os.path.join(MODEL_PATH, MODEL_NAME)\n",
        "if not os.path.exists(MODEL_DIR):\n",
        "    os.mkdir(MODEL_DIR)\n",
        "\n",
        "# if USE_DRIVE and not os.path.exists(MODEL_DIR):\n",
        "#   os.mkdir(MODEL_DIR)\n",
        "\n",
        "def fixed_loss_fn(x, scale=1e3, ax=[]):\n",
        "  return scale * torch.mean(torch.square(x[:, :4] - target[:4]), ax)\n",
        "\n",
        "def unsharp(img):\n",
        "  blured = gaussian_blur(img, (5, 5), (1, 1))\n",
        "  return img + (img - blured) * 2.\n",
        "\n",
        "s = np.sqrt(3) / 2.\n",
        "hex2xy = np.float32([[1., 0.], [.5, s]])\n",
        "xy2hex = torch.tensor(np.linalg.inv(hex2xy))\n",
        "r = torch.linspace(.5/W, 1, W//2)[:, None]\n",
        "a = torch.range(0, W*np.pi) / (W / 2)\n",
        "polar_xy = torch.stack([r*a.cos(), r*a.sin()], -1)[None, :]\n",
        "polar_target = F.grid_sample(unsharp(target[None, :]), polar_xy)\n",
        "\n",
        "x = torch.linspace(-1, 1, W)\n",
        "y, x = torch.meshgrid(x, x)\n",
        "xy_grid = torch.stack([x, y], -1)\n",
        "fft_target = torch.fft.rfft(polar_target).conj()\n",
        "polar_target_sqnorm = polar_target.square().sum(-1, keepdim=True)\n",
        "\n",
        "def invariant_losses_fn(img):\n",
        "  img = unsharp(img)\n",
        "  polar_img = F.grid_sample(img, polar_xy.repeat(len(img), 1, 1, 1), mode='bicubic')\n",
        "  x = torch.fft.rfft(polar_img)\n",
        "  xy = torch.fft.irfft(x*fft_target)\n",
        "  xx = polar_img.square().sum(-1, keepdim=True)\n",
        "  yy = polar_target_sqnorm\n",
        "  sqdiff = xx + yy - 2. * xy\n",
        "  return sqdiff.mean([1, 2])\n",
        "\n",
        "def invariant_loss_fn(img):\n",
        "  return invariant_losses_fn(img).min(-1)[0].mean()\n",
        "\n",
        "loss_fn = {\n",
        "    'FIXED': fixed_loss_fn,\n",
        "    'INVARIANT': invariant_loss_fn,\n",
        "}\n",
        "\n",
        "def circle_masks(n, sz):\n",
        "  x = torch.linspace(-1.0, 1.0, sz)[None, None, :]\n",
        "  y = torch.linspace(-1.0, 1.0, sz)[None, :, None]\n",
        "  center = -torch.rand([2, n, 1, 1]) + 0.5\n",
        "  r = -0.3*torch.rand([n, 1, 1]) + 0.4\n",
        "  x, y = (x-center[0])/r, (y-center[1])/r\n",
        "  mask = (x*x+y*y < 1.0).float()\n",
        "  return mask\n",
        "\n",
        "model = CA()\n",
        "loss_log = []\n",
        "progress = 0\n",
        "with torch.no_grad():\n",
        "  pool = seed(256, W, SEED_N, SEED_R)\n",
        "#   img = to_rgb(pool)\n",
        "#   imgs = img.permute([0, 2, 3, 1]).cpu()\n",
        "#   imshow(tile2d(imgs.detach(), 16), scale=1)\n",
        "opt = torch.optim.Adam(model.parameters(), UPPER_LR)\n",
        "lr_sched = torch.optim.lr_scheduler.CyclicLR(opt, LOWER_LR, UPPER_LR, step_size_up=2000, mode='triangular2', cycle_momentum=False)\n",
        "\n",
        "print ('model name: ', MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "id": "OFCqvxs1dr10"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/jpeg": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 1/15001 [00:07<33:15:40,  7.98s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step_n: 0 \tloss: 90.71288299560547 \tlr: 1.0495000000000166e-05"
          ]
        },
        {
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAIBAQEBAQIBAQECAgICAgQDAgICAgUEBAMEBgUGBgYFBgYGBwkIBgcJBwYGCAsICQoKCgoKBggLDAsKDAkKCgr/2wBDAQICAgICAgUDAwUKBwYHCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgoKCgr/wAARCABABAADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD9/KKKKACiiigAooooAKKKKAP4A6KKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA/v8ooooAKKKKACiorJ7ySzhfUbeKK4aJTPFBMZER8fMqsVUsAcgEqpI5wOlS02rOxMJKcFJde6af3PVej1CiiikUfwB0UUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQB/f5RRRQAUUUUAVNAm1250KyuPFOm2lnqclpG2o2en3rXMEE5UGSOOZ44mlRWyFdo4ywAJRSdot0UUkrIqclKbaVr9Fey8tbv722FFFFMk/gDooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigD+/yiiigAooooAKKKKACiiigD+AOiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAP/2Q==",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 2/15001 [00:11<21:45:35,  5.22s/it]"
          ]
        },
        {
          "data": {
            "image/jpeg": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 3/15001 [00:12<15:06:12,  3.63s/it]"
          ]
        },
        {
          "data": {
            "image/jpeg": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 4/15001 [00:17<15:47:44,  3.79s/it]"
          ]
        },
        {
          "data": {
            "image/jpeg": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 4/15001 [00:19<20:31:31,  4.93s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Marco\\Documents\\GitHub\\neural-cellular-automata\\2_blogpost_iso_nca\\blogpost_code.ipynb Cell 9\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Marco/Documents/GitHub/neural-cellular-automata/2_blogpost_iso_nca/blogpost_code.ipynb#X11sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mparameters():\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Marco/Documents/GitHub/neural-cellular-automata/2_blogpost_iso_nca/blogpost_code.ipynb#X11sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     p\u001b[39m.\u001b[39mgrad \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m (p\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mnorm() \u001b[39m+\u001b[39m \u001b[39m1e-8\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Marco/Documents/GitHub/neural-cellular-automata/2_blogpost_iso_nca/blogpost_code.ipynb#X11sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m opt\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Marco/Documents/GitHub/neural-cellular-automata/2_blogpost_iso_nca/blogpost_code.ipynb#X11sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m opt\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Marco/Documents/GitHub/neural-cellular-automata/2_blogpost_iso_nca/blogpost_code.ipynb#X11sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m lr_sched\u001b[39m.\u001b[39mstep()\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:69\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     68\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[0;32m    133\u001b[0m         group,\n\u001b[0;32m    134\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    139\u001b[0m         state_steps)\n\u001b[1;32m--> 141\u001b[0m     adam(\n\u001b[0;32m    142\u001b[0m         params_with_grad,\n\u001b[0;32m    143\u001b[0m         grads,\n\u001b[0;32m    144\u001b[0m         exp_avgs,\n\u001b[0;32m    145\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    146\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    147\u001b[0m         state_steps,\n\u001b[0;32m    148\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    149\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    150\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    151\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    152\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    153\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    154\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    155\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    156\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    157\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    158\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    159\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 281\u001b[0m func(params,\n\u001b[0;32m    282\u001b[0m      grads,\n\u001b[0;32m    283\u001b[0m      exp_avgs,\n\u001b[0;32m    284\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    285\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    286\u001b[0m      state_steps,\n\u001b[0;32m    287\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    288\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    289\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    290\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    291\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    292\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    293\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    294\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    295\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    296\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    297\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:489\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    487\u001b[0m     torch\u001b[39m.\u001b[39m_foreach_addcdiv_(params_, device_exp_avgs, denom)\n\u001b[0;32m    488\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 489\u001b[0m     bias_correction1 \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m beta1 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m _get_value(step) \u001b[39mfor\u001b[39;49;00m step \u001b[39min\u001b[39;49;00m device_state_steps]\n\u001b[0;32m    490\u001b[0m     bias_correction2 \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2 \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m _get_value(step) \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m device_state_steps]\n\u001b[0;32m    492\u001b[0m     step_size \u001b[39m=\u001b[39m _stack_if_compiling([(lr \u001b[39m/\u001b[39m bc) \u001b[39m*\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m bc \u001b[39min\u001b[39;00m bias_correction1])\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\optim\\adam.py:489\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    487\u001b[0m     torch\u001b[39m.\u001b[39m_foreach_addcdiv_(params_, device_exp_avgs, denom)\n\u001b[0;32m    488\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 489\u001b[0m     bias_correction1 \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1 \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m _get_value(step) \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m device_state_steps]\n\u001b[0;32m    490\u001b[0m     bias_correction2 \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2 \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m _get_value(step) \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m device_state_steps]\n\u001b[0;32m    492\u001b[0m     step_size \u001b[39m=\u001b[39m _stack_if_compiling([(lr \u001b[39m/\u001b[39m bc) \u001b[39m*\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39mfor\u001b[39;00m bc \u001b[39min\u001b[39;00m bias_correction1])\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#@title Training Loop {vertical-output:true}\n",
        "\n",
        "TRAIN_MODEL = True\n",
        "EPOCHS = 15000  #@param{type:\"number\"}\n",
        "DAMAGE_RATE = 3\n",
        "IMAGE_RATE = 20\n",
        "INFO_RATE = 20\n",
        "SAVE_RATE = 500\n",
        "\n",
        "for _ in tqdm(range(EPOCHS+1)):\n",
        "    if not TRAIN_MODEL:\n",
        "        print ('skipping training')\n",
        "        break\n",
        "    with torch.no_grad():\n",
        "        i = len(loss_log)\n",
        "        batch_idx = np.random.choice(len(pool), 8, replace=False)\n",
        "        x = pool[batch_idx]\n",
        "        # loss_rank = torch.argsort(loss_fn(x, ax=[-2, -3, -1]), descending=True)\n",
        "        # x = x[loss_rank]\n",
        "        \n",
        "        seed_rate = 1  if i < 4000 else 5\n",
        "        if i % seed_rate == 0:\n",
        "            x[:1] = seed(1, W, SEED_N, SEED_R)\n",
        "\n",
        "        if i % DAMAGE_RATE == 0:\n",
        "            damage_mask = 1. - circle_masks(1, W)[:, None]\n",
        "            x[-1:] *= damage_mask\n",
        "\n",
        "    step_n = np.random.randint(64, 96)\n",
        "    overflow_loss = 0.\n",
        "    diff_loss = 0.\n",
        "    target_loss = 0.\n",
        "    last_x = torch.zeros(x.shape)\n",
        "    for _ in range(step_n):\n",
        "        px = x\n",
        "        x = model(x)\n",
        "        diff_loss += (x - px).abs().mean()\n",
        "        overflow_loss += (x - x.clamp(-2., 2.))[:, :SCALAR_CHN].square().sum()\n",
        "\n",
        "    target_loss += loss_fn[LOSS_FN](x[:, :target.shape[0]])\n",
        "    diff_loss *= 10.\n",
        "    loss = target_loss + overflow_loss + diff_loss\n",
        "    # if loss.isnan():\n",
        "    #   # TODO: reload model from last checkpoint\n",
        "    #   print('\\nWARNING: NaN')\n",
        "    #   pool[batch_idx] = seed(8, W, SEED_N, SEED_R)\n",
        "    #   continue\n",
        "\n",
        "    with torch.no_grad():\n",
        "        loss.backward()\n",
        "        for p in model.parameters():\n",
        "            p.grad /= (p.grad.norm() + 1e-8)\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "        lr_sched.step()\n",
        "        pool[batch_idx] = x\n",
        "        loss_log.append(loss.item())\n",
        "\n",
        "    if i % IMAGE_RATE == 0:\n",
        "        clear_output(True)\n",
        "        pl.plot(loss_log, '.', alpha=.1)\n",
        "        pl.yscale('log')\n",
        "        pl.ylim(np.min(loss_log), loss_log[0])\n",
        "        pl.show()\n",
        "        imgs = to_rgb(x)\n",
        "        imgs = imgs.permute([0, 2, 3, 1]).cpu()\n",
        "        imshow(tile2d(imgs.detach(), 4), scale=2)\n",
        "\n",
        "    if i % INFO_RATE == 0:\n",
        "        print('\\rstep_n:', i, '\\tloss:', loss.item(), '\\tlr:', lr_sched.get_lr()[0], end='')\n",
        "\n",
        "    if i % SAVE_RATE == 0:\n",
        "        progress = i\n",
        "        torch.save(model, os.path.join(MODEL_DIR, f'{MODEL_NAME}_{i}.pt'))\n",
        "\n",
        "if TRAIN_MODEL: \n",
        "    # * save final model\n",
        "    torch.save(model, os.path.join('_models', f'{MODEL_NAME}.pt'))\n",
        "    torch.save(model.state_dict(), os.path.join('_states', f'{MODEL_NAME}_state_dict.pt'))\n",
        "\n",
        "    # * create model video!\n",
        "    FRAMES = 500  #@param{type:\"integer\"}\n",
        "    progress = 15000\n",
        "\n",
        "    model_name = f'{MODEL_TYPE}_{TARGET}_{LOSS_FN}_{SEED_N}pt_{SEED_R}r_{progress}'.lower()\n",
        "    model_path = os.path.join(MODEL_DIR, f'{model_name}.pt')\n",
        "    demo_path = os.path.join('_videos', f'{model_name}.mp4')\n",
        "\n",
        "    try:\n",
        "        model = torch.load(model_path)\n",
        "        vidgen(demo_path, model, n_frames=FRAMES, p=SEED_N, r=SEED_R)\n",
        "    except FileNotFoundError:\n",
        "        print(f'Model \"{model_name}\" not found.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Utility functions for running pygame instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import PIL.Image\n",
        "\n",
        "# * loads an image and converts to a tensor\n",
        "# *     default tensor shape: [BATCH_SIZE (1), CHANNELS (4), WIDTH (_size), HEIGHT (_size)]\n",
        "def load_image_as_tensor(_path, _size, _resample=PIL.Image.Resampling.BICUBIC):\n",
        "    img = PIL.Image.open(_path)\n",
        "    img = img.resize((_size, _size), _resample)\n",
        "    img = np.float32(img) / 255.0\n",
        "    img[..., :3] *= img[..., 3:]\n",
        "    return torch.from_numpy(img).permute(2, 0, 1)[None, ...]\n",
        "\n",
        "# * takes the first 4 channels of a tensor of default shape and converts to a RGB image\n",
        "def to_rgb(_x, _alpha='BLACK'):\n",
        "    rgb, a = _x[:, :3], _x[:, 3:4]\n",
        "    if _alpha == 'BLACK':\n",
        "        return torch.clamp(rgb, 0.0, 1.0)\n",
        "    elif _alpha == 'WHITE':\n",
        "        return torch.clamp(1.-a + rgb, 0.0, 1.0)\n",
        "    \n",
        "# * creates a circle mask centered at a position of a given radius\n",
        "def circle_mask(_size, _radius, _pos):\n",
        "    Y, X = np.ogrid[:_size, :_size]\n",
        "    dist_from_center = np.sqrt((X - _pos[0])**2 + (Y-_pos[1])**2)\n",
        "    mask = dist_from_center >= _radius\n",
        "    return mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pygame\n",
        "import datetime\n",
        "import os\n",
        "import cv2\n",
        "import imutils\n",
        "import torch.nn.functional as func\n",
        "import torchvision.transforms.functional as trans\n",
        "\n",
        "_PLAY_ = False\n",
        "_MODELS_DIR_ = '_states'\n",
        "_PLAY_DEVICE_ = 'cpu'\n",
        "_RADIUS_ = 8\n",
        "_SEED_SIZE_ = 64\n",
        "_PLAY_SIZE_ = 128\n",
        "_WINDOW_SCALE_ = 8\n",
        "_WINDOW_BG_COLOR_ = (0, 0, 0)\n",
        "_WINDOW_TEXT_COLOR_ = (255, 255, 255)\n",
        "\n",
        "# * set current device\n",
        "_DEVICE_ = _PLAY_DEVICE_\n",
        "torch.set_default_tensor_type('torch.FloatTensor')\n",
        "\n",
        "# * method to load model for play\n",
        "def load_model(_model_name):\n",
        "     # * find params from name\n",
        "    params = _model_name.split('_')\n",
        "    \n",
        "    # * set global vars\n",
        "    global MODEL_TYPE\n",
        "    global ANGLE_CHN\n",
        "    global SCALAR_CHN\n",
        "    global LOSS_FN\n",
        "    MODEL_TYPE = params[0].upper()\n",
        "    ANGLE_CHN = 1 if MODEL_TYPE == 'STEERABLE' else 0\n",
        "    SCALAR_CHN = CHN - ANGLE_CHN\n",
        "    LOSS_FN = params[2].upper()\n",
        "    \n",
        "    # * load model\n",
        "    model = CA()\n",
        "    model.load_state_dict(torch.load(os.path.join(_MODELS_DIR_, _model_name+'.pt'), map_location=_PLAY_DEVICE_))\n",
        "    model.eval()\n",
        "    \n",
        "    # * create seed and tensor\n",
        "    seed_img = load_image_as_tensor('..\\\\_seeds\\\\'+seeds_list[curr_seed], _SEED_SIZE_)\n",
        "    pad = np.round((_PLAY_SIZE_-_SEED_SIZE_)/2.0, 0).astype(int)\n",
        "    seed_img = func.pad(seed_img, (pad, pad, pad, pad), 'constant', 0)\n",
        "    tensor = torch.cat([seed_img, seed_img[:, 3].repeat(1, 12, 1, 1).to(_PLAY_DEVICE_)], 1).to(_PLAY_DEVICE_)\n",
        "    # * randomize angles for steerable models\n",
        "    if params[0] == 'steerable':\n",
        "        rand = torch.rand(_PLAY_SIZE_, _PLAY_SIZE_)*np.pi*2.0\n",
        "        tensor[:, -1:] = rand\n",
        "    return model, tensor, params, seed_img\n",
        "\n",
        "# * get list of seeds\n",
        "seeds_list = os.listdir('..\\\\_seeds')\n",
        "seeds_list = [item for item in seeds_list if item.endswith('.png')]\n",
        "print ('seeds: ', seeds_list)\n",
        "curr_seed = 2\n",
        "\n",
        "# * get list of models\n",
        "model_list = os.listdir(_MODELS_DIR_)\n",
        "model_list = [item.replace('.pt', '') for item in model_list if item.endswith('.pt')]\n",
        "print ('models: ', model_list)\n",
        "curr_model = 1\n",
        "\n",
        "# load first model\n",
        "model, tensor, params, seed_img = load_model(model_list[curr_model])\n",
        "\n",
        "# * misc params\n",
        "angle = 0.0\n",
        "fps = 0\n",
        "show_vecs = False\n",
        "bg_color = 'WHITE' if _WINDOW_BG_COLOR_ == (255, 255, 255) else 'BLACK'\n",
        "prev_time = datetime.datetime.now()\n",
        "\n",
        "# * load vector image\n",
        "vec_img = cv2.imread('..\\\\_images\\\\vector_v3.png') \n",
        "vec_img = cv2.resize(vec_img, (_WINDOW_SCALE_, _WINDOW_SCALE_))\n",
        "vec_img = vec_img.astype(float)/255.0\n",
        "\n",
        "# * start pygame\n",
        "pygame.init()\n",
        "pygame.display.set_caption('nca play - '+model_list[curr_model])\n",
        "\n",
        "# * model dependent params\n",
        "size = _PLAY_SIZE_\n",
        "window_size = size * _WINDOW_SCALE_\n",
        "window = pygame.display.set_mode((window_size, window_size))\n",
        "\n",
        "# * text renders\n",
        "font_size = 20\n",
        "my_font = pygame.font.SysFont('consolas', font_size)\n",
        "model_surface = my_font.render('[UP/DOWN] model: ' + model_list[curr_model], False, _WINDOW_TEXT_COLOR_)\n",
        "seed_surface = my_font.render('[LEFT/RIGHT] seed: ' + seeds_list[curr_seed], False, _WINDOW_TEXT_COLOR_)\n",
        "text_surface = my_font.render('[SCROLL] angle: ' + str(angle) + 'œÄ', False, _WINDOW_TEXT_COLOR_)\n",
        "fps_surface = my_font.render('fps: ' + str(int(fps)), False, _WINDOW_TEXT_COLOR_)\n",
        "vec_surface = my_font.render('[V] show vectors: ' + str(show_vecs), False, _WINDOW_TEXT_COLOR_)\n",
        "\n",
        "# * start infinite game loop\n",
        "running = True\n",
        "mouse_down = False\n",
        "model_start = False\n",
        "while running:\n",
        "    if not _PLAY_:\n",
        "        print ('skipping game')\n",
        "        break\n",
        "    # empty cache\n",
        "    torch.cuda.empty_cache()\n",
        "    # handle events\n",
        "    for event in pygame.event.get():\n",
        "        if event.type == pygame.QUIT:\n",
        "            running = False\n",
        "        if event.type == pygame.KEYDOWN:\n",
        "            # * close application\n",
        "            if event.key == pygame.K_ESCAPE:\n",
        "                running = False\n",
        "                break\n",
        "            # * toggle showing vectors\n",
        "            if event.key == pygame.K_v:\n",
        "                show_vecs = not show_vecs\n",
        "                vec_surface = my_font.render('[V] show vectors: ' + str(show_vecs), False, _WINDOW_TEXT_COLOR_)\n",
        "            # * start current model\n",
        "            if event.key == pygame.K_SPACE:\n",
        "                model_start = not model_start\n",
        "            # * reset current model\n",
        "            if event.key == pygame.K_r:\n",
        "                model_start = False\n",
        "                seed_img_rot = trans.rotate(seed_img, np.rad2deg(angle*np.pi))\n",
        "                tensor = torch.cat([seed_img_rot, seed_img_rot[:, 3].repeat(1, 12, 1, 1).to(_PLAY_DEVICE_)], 1).to(_PLAY_DEVICE_)\n",
        "                # * randomize angles for steerable models\n",
        "                if params[0] == 'steerable':\n",
        "                    rand = torch.rand(_PLAY_SIZE_, _PLAY_SIZE_)*np.pi*2.0\n",
        "                    tensor[:, -1:] = rand\n",
        "            # * use up/down arrow keys to cycle though models\n",
        "            if event.key == pygame.K_UP:\n",
        "                    curr_model += 1\n",
        "                    if curr_model >= len(model_list):\n",
        "                        curr_model = 0\n",
        "            if event.key == pygame.K_DOWN:\n",
        "                curr_model -= 1\n",
        "                if curr_model < 0:\n",
        "                    curr_model = len(model_list)-1\n",
        "            # * load new model\n",
        "            if event.key == pygame.K_UP or event.key == pygame.K_DOWN:\n",
        "                model_start = False\n",
        "                model, tensor, params, seed_img = load_model(model_list[curr_model])\n",
        "                pygame.display.set_caption('nca play - '+model_list[curr_model])\n",
        "                model_surface = my_font.render('[UP/DOWN] model: '+model_list[curr_model], False, _WINDOW_TEXT_COLOR_)\n",
        "                seed_surface = my_font.render('[LEFT/RIGHT] seed: '+seeds_list[curr_seed], False, _WINDOW_TEXT_COLOR_)\n",
        "            # * use left/right arrow keys to cycle though seeds\n",
        "            if event.key == pygame.K_LEFT:\n",
        "                curr_seed += 1\n",
        "                if curr_seed >= len(seeds_list):\n",
        "                    curr_seed = 0\n",
        "            if event.key == pygame.K_RIGHT:\n",
        "                curr_seed -= 1\n",
        "                if curr_seed < 0:\n",
        "                    curr_seed = len(seeds_list)-1\n",
        "            # * load new seed image\n",
        "            if event.key == pygame.K_LEFT or event.key == pygame.K_RIGHT:\n",
        "                model_start = False\n",
        "                seed_img = load_image_as_tensor('..\\\\_seeds\\\\'+seeds_list[curr_seed], _SEED_SIZE_)\n",
        "                pad = np.round((_PLAY_SIZE_-_SEED_SIZE_)/2.0, 0).astype(int)\n",
        "                seed_img = func.pad(seed_img, (pad, pad, pad, pad), 'constant', 0)\n",
        "                tensor = torch.cat([seed_img, seed_img[:, 3].repeat(1, 12, 1, 1).to(_PLAY_DEVICE_)], 1).to(_PLAY_DEVICE_)\n",
        "                # * randomize angles for steerable models\n",
        "                if params[0] == 'steerable':\n",
        "                    rand = torch.rand(_PLAY_SIZE_, _PLAY_SIZE_)*np.pi*2.0\n",
        "                    tensor[:, -1:] = rand\n",
        "                seed_surface = my_font.render('[LEFT/RIGHT] seed: '+seeds_list[curr_seed], False, _WINDOW_TEXT_COLOR_)\n",
        "        if event.type == pygame.MOUSEWHEEL:\n",
        "            # * let player rotate seed before starting model\n",
        "            if not model_start:\n",
        "                angle = np.round((event.y * 0.05) + angle, decimals=2)\n",
        "                text_surface = my_font.render('[SCROLL] angle: ' + str(angle) + 'œÄ', False, _WINDOW_TEXT_COLOR_)\n",
        "                seed_img_rot = trans.rotate(seed_img, np.rad2deg(angle*np.pi))\n",
        "                tensor = torch.cat([seed_img_rot, seed_img_rot[:, 3].repeat(1, 12, 1, 1).to(_PLAY_DEVICE_)], 1).to(_PLAY_DEVICE_)\n",
        "                # * randomize angles for steerable models\n",
        "                if params[0] == 'steerable':\n",
        "                    rand = torch.rand(_PLAY_SIZE_, _PLAY_SIZE_)*np.pi*2.0\n",
        "                    tensor[:, -1:] = rand\n",
        "        # * mouse click events - erase and draw\n",
        "        if event.type == pygame.MOUSEBUTTONDOWN:\n",
        "            mouse_down = True\n",
        "            if pygame.mouse.get_pressed(3)[2] and model_start:\n",
        "                mouse = np.array(pygame.mouse.get_pos(), dtype=float)\n",
        "                pos = mouse / window_size * size\n",
        "                dot = np.zeros_like(tensor.detach().cpu().numpy())\n",
        "                dot[:, 3:, int(pos[1]), int(pos[0])] = 1.0\n",
        "                tensor += torch.tensor(dot).to(_PLAY_DEVICE_)\n",
        "        if event.type == pygame.MOUSEBUTTONUP:\n",
        "            mouse_down = False\n",
        "        if (event.type == pygame.MOUSEMOTION or event.type == pygame.MOUSEBUTTONDOWN) and mouse_down and model_start:\n",
        "            mouse = np.array(pygame.mouse.get_pos(), dtype=float)\n",
        "            pos = mouse / window_size * size\n",
        "            if pygame.mouse.get_pressed(3)[0]:\n",
        "                mask = circle_mask(size, _RADIUS_, pos)\n",
        "                tensor[0:] *= torch.tensor(mask).to(_PLAY_DEVICE_)\n",
        "            \n",
        "    # * update tensor\n",
        "    if model_start:\n",
        "        with torch.no_grad():\n",
        "            tensor = model(tensor)\n",
        "    \n",
        "    # * draw tensor to window\n",
        "    window.fill(_WINDOW_BG_COLOR_)\n",
        "    img = to_rgb(tensor, _alpha=bg_color).squeeze()\n",
        "    vis = (np.array(img.cpu())*255).astype(np.uint8)\n",
        "    if show_vecs:\n",
        "        vecs = tensor[:, -1:].squeeze(0)\n",
        "        vecs = np.array(vecs.cpu())%(2.0*torch.pi)\n",
        "    pixel = pygame.Surface((_WINDOW_SCALE_, _WINDOW_SCALE_))\n",
        "    for j in range(size):\n",
        "        for i in range(size):\n",
        "            color = vis[:, i, j]\n",
        "            # * create vectors for each cell\n",
        "            if show_vecs:\n",
        "                vec_dir = vecs[:, i, j]\n",
        "                vec = imutils.rotate(vec_img, angle=np.rad2deg(vec_dir).item())\n",
        "                vec *= color\n",
        "                surf = pygame.surfarray.make_surface(vec)\n",
        "                pixel.blit(surf, (0, 0))\n",
        "            # * fill cell with color\n",
        "            else:\n",
        "                pixel.fill(color)\n",
        "            draw_me = pygame.Rect(j*_WINDOW_SCALE_, i*_WINDOW_SCALE_, _WINDOW_SCALE_, _WINDOW_SCALE_)\n",
        "            window.blit(pixel, draw_me)\n",
        "    \n",
        "    # * calculate fps\n",
        "    now = datetime.datetime.now()\n",
        "    if (now - prev_time).seconds >= 1.0:\n",
        "        prev_time = now\n",
        "        fps_surface = my_font.render('fps: ' + str(int(fps)), False, _WINDOW_TEXT_COLOR_)\n",
        "        fps = 0\n",
        "    else:\n",
        "        fps += 1       \n",
        "    \n",
        "    # * render text\n",
        "    window.blit(model_surface, (0, 0))\n",
        "    window.blit(seed_surface, (0, font_size))\n",
        "    window.blit(text_surface, (0, window_size-font_size))\n",
        "    window.blit(vec_surface, (0, window_size-font_size*2))\n",
        "    window.blit(fps_surface, (0, window_size-font_size*3))\n",
        "    \n",
        "    # * flip it!\n",
        "    pygame.display.flip()\n",
        "\n",
        "# * quit it!\n",
        "pygame.quit()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
