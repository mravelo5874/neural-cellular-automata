{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "974P6JcnyfPf"
      },
      "source": [
        "# Isotropic and Steerable NCA (structured seed experiments)\n",
        "\n",
        "### Author: Craig Fouts (cwf2117@columbia.edu)\n",
        "\n",
        "*Copyright 2023 Craig Fouts*\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "[https://www.apache.org/licenses/LICENSE-2.0](https://www.apache.org/licenses/LICENSE-2.0)\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "NCTKsrsiOUiG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU 0: NVIDIA GeForce GTX 1660 Ti (UUID: GPU-d77bcb98-b41a-4494-1c93-f4f7d0b55d9c)\n",
            "cuda available?  True\n",
            "device:  cuda\n"
          ]
        }
      ],
      "source": [
        "#@title Notebook Utilities and Setup\n",
        "\n",
        "import base64\n",
        "import io\n",
        "import matplotlib.pylab as pl\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL.Image, PIL.ImageDraw, PIL.ImageFont\n",
        "import requests\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms.functional as T\n",
        "import warnings\n",
        "from colorsys import hsv_to_rgb\n",
        "# from google.colab import drive, output\n",
        "from IPython.display import clear_output, Image\n",
        "from torch.nn import BatchNorm1d, Dropout, InstanceNorm1d, LayerNorm, Module, ReLU, Sequential\n",
        "from torchvision.transforms.functional_tensor import gaussian_blur\n",
        "from tqdm.notebook import tnrange\n",
        "from tqdm import tqdm\n",
        "\n",
        "os.environ['FFMPEG_BINARY'] = 'ffmpeg'\n",
        "import moviepy.editor as mvp\n",
        "from moviepy.video.io.ffmpeg_writer import FFMPEG_VideoWriter\n",
        "\n",
        "# output.enable_custom_widget_manager()\n",
        "\n",
        "# * find GPU available\n",
        "clear_output()\n",
        "!nvidia-smi -L\n",
        "\n",
        "# * sets the device\n",
        "# *     defaults to 'cuda'\n",
        "_DEVICE_ = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print ('cuda available? ', torch.cuda.is_available())\n",
        "print ('device: ', _DEVICE_)\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.cuda.empty_cache()\n",
        "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# USE_DRIVE = False  #@param{type:\"boolean\"}\n",
        "DIRECTORY = 'My Drive/Models'  #@param{type:\"string\"}\n",
        "MODEL_PATH = '_models'\n",
        "\n",
        "# if USE_DRIVE:\n",
        "#   drive.mount('/content/drive')\n",
        "#   MODEL_PATH = os.path.join('/content/drive', DIRECTORY)\n",
        "\n",
        "def imread(url, max_size=None, mode=None):\n",
        "  if url.startswith(('http:', 'https:')):\n",
        "    headers = {'User-Agent': 'Requests in Colab/0.0 (https://colab.research.google.com/; no-reply@google.com) requests/0.0'}\n",
        "    r = requests.get(url, headers=headers)\n",
        "    f = io.BytesIO(r.content)\n",
        "  else:\n",
        "    f = url\n",
        "  img = PIL.Image.open(f)\n",
        "  if max_size is not None:\n",
        "    img.thumbnail((max_size, max_size), PIL.Image.ANTIALIAS)\n",
        "  if mode is not None:\n",
        "    img = img.convert(mode)\n",
        "  img = np.float32(img) / 255.\n",
        "  return img\n",
        "\n",
        "def np2pil(a):\n",
        "  if a.dtype in (np.float32, np.float64):\n",
        "    a = np.uint8(np.clip(a, 0, 1)*255)\n",
        "  return PIL.Image.fromarray(a)\n",
        "\n",
        "def imwrite(f, a, fmt=None, quality=95):\n",
        "  a = np.asarray(a)\n",
        "  if isinstance(f, str):\n",
        "    fmt = f.rsplit('.', 1)[-1].lower()\n",
        "    if fmt == 'jpg':\n",
        "      fmt = 'jpeg'\n",
        "    f = open(f, 'wb')\n",
        "  np2pil(a).save(f, fmt, quality=quality)\n",
        "\n",
        "def imencode(a, fmt='jpeg'):\n",
        "  a = np.asarray(a)\n",
        "  if a.ndim == 3 and a.shape[-1] == 4:\n",
        "    fmt = 'png'\n",
        "  f = io.BytesIO()\n",
        "  imwrite(f, a, fmt)\n",
        "  return f.getvalue()\n",
        "\n",
        "def im2url(a, fmt='jpeg'):\n",
        "  encoded = imencode(a, fmt)\n",
        "  base64_byte_string = base64.b64encode(encoded).decode('ascii')\n",
        "  return 'data:image/' + fmt.upper() + ';base64,' + base64_byte_string\n",
        "\n",
        "def zoom(img, scale=4):\n",
        "  img = np.repeat(img, scale, 0)\n",
        "  img = np.repeat(img, scale, 1)\n",
        "  return img\n",
        "\n",
        "def imshow(a, fmt='jpeg', scale=4):\n",
        "  display(Image(data=imencode(zoom(a, scale), fmt)))\n",
        "\n",
        "def tile2d(a, w=None):\n",
        "  a = np.asarray(a)\n",
        "  if w is None:\n",
        "    w = int(np.ceil(np.sqrt(a.shape[0])))\n",
        "  th, tw = a.shape[1:3]\n",
        "  pad = (w - a.shape[0]) % w\n",
        "  a = np.pad(a, [(0, pad)]+[(0, 0)]*(a.ndim-1), 'constant')\n",
        "  h = a.shape[0] // w\n",
        "  a = a.reshape([h, w]+list(a.shape[1:]))\n",
        "  a = np.rollaxis(a, 2, 1).reshape([th*h, tw*w]+list(a.shape[4:]))\n",
        "  return a\n",
        "\n",
        "def to_rgb(x):\n",
        "  rgb, a = x[:, :3], x[:, 3:4]\n",
        "  return 1. - a + rgb\n",
        "\n",
        "def grab_plot(close=True):\n",
        "  fig = pl.gcf()\n",
        "  fig.canvas.draw()\n",
        "  img = np.array(fig.canvas.renderer._renderer)\n",
        "  a = np.float32(img[..., 3:]/255.)\n",
        "  img = np.uint8(255*(1.-a)+img[..., :3]*a)\n",
        "  if close:\n",
        "    pl.close()\n",
        "  return img\n",
        "\n",
        "def vis_angle(x, w):\n",
        "  m = get_alive_mask(x).cpu()\n",
        "  rgb = to_rgb(x)[0].clip(0, 1).permute(1, 2, 0).cpu()\n",
        "  ang, a, m = x[0, -1].cpu(), x[0, 3].cpu(), m[0,0]\n",
        "  c, s = ang.cos() * a, ang.sin() * a\n",
        "  px, py = np.mgrid[-1:1:w*1j, -1:1:w*1j]\n",
        "  pl.figure(figsize=(10, 10))\n",
        "  pl.axis('equal')\n",
        "  pl.xlim(-0.8, 0.8); pl.ylim(-0.8,0.8)\n",
        "  pl.quiver(px[m], py[m], c[m], s[m], color=rgb[m], pivot='mid', scale_units='xy', units='xy', scale=W/3)\n",
        "  pl.tight_layout()\n",
        "  pl.axis('off')\n",
        "  return grab_plot()\n",
        "\n",
        "class VideoWriter:\n",
        "  def __init__(self, filename='_autoplay.mp4', fps=30., **kwargs):\n",
        "    self.writer = None\n",
        "    self.params = dict(filename=filename, fps=fps, **kwargs)\n",
        "\n",
        "  def __enter__(self):\n",
        "    return self\n",
        "  \n",
        "  def __exit__(self, *args):\n",
        "    self.close()\n",
        "    if self.params['filename'] == '_autoplay.mp4':\n",
        "      self.show()\n",
        "\n",
        "  def add(self, img):\n",
        "    img = np.asarray(img)\n",
        "    if self.writer is None:\n",
        "      h, w = img.shape[:2]\n",
        "      self.writer = FFMPEG_VideoWriter(size=(w, h), **self.params)\n",
        "    if img.dtype in (np.float32, np.float64):\n",
        "      img = np.uint8(img.clip(0, 1)*255)\n",
        "    if img.ndim == 2:\n",
        "      img = np.repeat(img[..., None], 3, -1)\n",
        "    self.writer.write_frame(img)\n",
        "\n",
        "  def close(self):\n",
        "    if self.writer is not None:\n",
        "      self.writer.close()\n",
        "\n",
        "  def show(self, **kwargs):\n",
        "    self.close()\n",
        "    fn = self.params['filename']\n",
        "    display(mvp.ipython_display(fn, **kwargs))\n",
        "\n",
        "def vidgen(filename, model, n_frames=500, max_speed=128, n=16, sz=72, p=2, r=4, angle=None):\n",
        "  with VideoWriter(filename=filename) as vid, torch.no_grad():\n",
        "    x = seed(n, sz, p, r, angle=angle)\n",
        "    for i in tnrange(n_frames, leave=False):\n",
        "      img = to_rgb(x).permute(0, 2, 3, 1).cpu()\n",
        "      vid.add(zoom(tile2d(img), 2))\n",
        "      step_n = min(2**(i//30), max_speed)\n",
        "      for _ in range(step_n):\n",
        "        x = model(x)\n",
        "    vid.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4azYa9u3aMT"
      },
      "source": [
        "# Model Legend\n",
        "\n",
        "Select the model of interest from the following\n",
        "\n",
        "* LAPLACIAN:&emsp;Isotropic NCA model\n",
        "* LAP6:&emsp;Isotropic NCA, (trained and/or evaluated) on a hexagonal grid\n",
        "* GRADNORM:&emsp;Isotropic NCA variant discussed in the blogpost\n",
        "* STEERABLE:&emsp;Angle-based Steerable NCA\n",
        "* GRADIENT:&emsp;Gradient-based Steerable NCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "08RFfJ5xl-cL"
      },
      "outputs": [],
      "source": [
        "#@title Model Utilities and Setup {vertical-output:true}\n",
        "\n",
        "MODEL_TYPE = 'STEERABLE'  #@param['LAPLACIAN', 'LAP6', 'GRADNORM', 'STEERABLE', 'GRADIENT']\n",
        "UPDATE_RATE = .5\n",
        "\n",
        "CHN = 16\n",
        "ANGLE_CHN = 1 if MODEL_TYPE == 'STEERABLE' else 0\n",
        "SCALAR_CHN = CHN - ANGLE_CHN\n",
        "\n",
        "IDENT = torch.tensor([[0., 0., 0.], [0., 1., 0.], [0., 0., 0.]])\n",
        "SOBEL = torch.tensor([[-1., 0., 1.], [-2., 0., 2.], [-1., 0., 1.]])\n",
        "LAP = torch.tensor([[1., 2., 1.], [2., -12., 2.], [1., 2., 1.]])\n",
        "LAP6 = torch.tensor([[0., 2., 2.], [2., -12., 2.], [2., 2., 0.]])\n",
        "GAUSS = torch.tensor([[1., 2., 1.], [2., 4., 2.], [1., 2., 1.]]) / 16.\n",
        "NHOOD_KERNEL = ((LAP6 if MODEL_TYPE == 'LAP6' else LAP) != 0.).to(torch.float32)\n",
        "\n",
        "def perchannel_conv(x, filters):\n",
        "  b, ch, h, w = x.shape\n",
        "  y = x.reshape(b * ch, 1, h, w)\n",
        "  y = F.pad(y, (1, 1, 1, 1), 'circular')\n",
        "  y = F.conv2d(y, filters[:, None])\n",
        "  return y.reshape(b, -1, h, w)\n",
        "\n",
        "# Isotropic models\n",
        "def laplacian_perception(x):\n",
        "  state_lap = perchannel_conv(x, LAP[None, :])\n",
        "  return torch.cat([x, state_lap], 1)\n",
        "\n",
        "def lap6_perception(x):\n",
        "  state_lap = perchannel_conv(x, LAP6[None, :])\n",
        "  return torch.cat([x, state_lap], 1)\n",
        "\n",
        "def gradnorm_perception(x):\n",
        "  grad = perchannel_conv(x, torch.stack([SOBEL, SOBEL.T]))\n",
        "  gx, gy = grad[:, ::2], grad[:, 1::2]\n",
        "  state_lap = perchannel_conv(x, LAP[None, :])\n",
        "  return torch.cat([x, state_lap, (gx*gx+gy*gy+1e-8).sqrt()], 1)\n",
        "\n",
        "# Steerable models\n",
        "def steerable_perception(x):\n",
        "  state, angle = x[:, :-1], x[:, -1:]\n",
        "  c, s = angle.cos(), angle.sin()\n",
        "  filters = torch.stack([SOBEL, SOBEL.T])\n",
        "  grad = perchannel_conv(state, filters)\n",
        "  gx, gy = grad[:, ::2], grad[:, 1::2]\n",
        "  rot_grad = torch.cat([gx*c+gy*s, gy*c-gx*s], 1)\n",
        "  state_lap = perchannel_conv(state, LAP[None, :])\n",
        "  return torch.cat([state, rot_grad, state_lap], 1)\n",
        "\n",
        "def gradient_perception(x):\n",
        "  filters = torch.stack([SOBEL, SOBEL.T])\n",
        "  grad = perchannel_conv(x, filters)\n",
        "  grad, dir = grad[:, :-2], grad[:, -2:]\n",
        "  dir = dir / dir.norm(dim=1, keepdim=True).clip(1.)\n",
        "  c, s = dir[:, :1], dir[:, 1:2]\n",
        "  gx, gy = grad[:, ::2], grad[:, 1::2]\n",
        "  rot_grad = torch.cat([gx*c+gy*s, gy*c-gx*s], 1)\n",
        "  state_lap = perchannel_conv(x, LAP[None, :])\n",
        "  return torch.cat([x, state_lap, rot_grad], 1)\n",
        "\n",
        "perception = {\n",
        "    'LAPLACIAN': laplacian_perception,\n",
        "    'LAP6': lap6_perception,\n",
        "    'GRADNORM': gradnorm_perception,\n",
        "    'STEERABLE': steerable_perception,\n",
        "    'GRADIENT': gradient_perception\n",
        "}[MODEL_TYPE]\n",
        "\n",
        "def get_alive_mask(x):\n",
        "  mature = (x[:, 3:4] > .1).to(torch.float32)\n",
        "  return perchannel_conv(mature, NHOOD_KERNEL[None, :]) > .5\n",
        "\n",
        "def fibonacci_lattice(n):\n",
        "  '''Generates an n-point fibonacci lattice of radius 1'''\n",
        "  epsilon = 0.33  # Assumes n < 24\n",
        "  golden_ratio = (1 + np.sqrt(5)) / 2.\n",
        "  pts = torch.arange(n)\n",
        "  theta = 2 * np.pi * pts / golden_ratio\n",
        "  phi = torch.arccos(1-2*(pts+epsilon)/(n-1+2*epsilon))\n",
        "  x, y, z = torch.cos(theta)*torch.sin(phi), torch.sin(theta)*torch.sin(phi), torch.cos(phi)\n",
        "  return torch.concat([x[None, :], y[None, :], z[None, :]], 0)\n",
        "\n",
        "def rgb_linspace(n):\n",
        "  '''Generates n visually distinct rgb combinations'''\n",
        "  return torch.tensor([hsv_to_rgb(i / n, 1.0, 1.0) for i in range(n)], dtype=torch.float32)\n",
        "\n",
        "def rotate_n(x, n, min=0., max=360.):\n",
        "  a = np.linspace(0., 360., n)\n",
        "  for i, a in zip(range(n), a):\n",
        "    x[i] = T.rotate(x[i], a)\n",
        "  return x\n",
        "\n",
        "def seed(n, sz=128, p=2, r=4, xy=None, angle=0., flip=False, rgb_dist=rgb_linspace):\n",
        "  '''Generates a uniform p-point structured seed of radius r'''\n",
        "  x = torch.zeros(n, CHN, sz, sz)\n",
        "  if SCALAR_CHN != CHN:\n",
        "    x[:, -1] = torch.rand(n, sz, sz) * np.pi * 2.\n",
        "  # Initialize p points equidistant around a circle of radius r\n",
        "  t = np.linspace(0, 2 * np.pi, p, endpoint=False)\n",
        "  if xy is None:\n",
        "    xy = (np.c_[r*np.cos(t), r*np.sin(t)]+(sz//2)).astype(np.int32).T\n",
        "  # Assign distinct rgb values to each point\n",
        "  x[:, :3, xy[0], xy[1]] = rgb_dist(xy.shape[1]).T\n",
        "  x[:, 3:SCALAR_CHN, xy[0], xy[1]] = 1.0\n",
        "  x = rotate_n(x, n) if angle is None else T.rotate(x, angle)\n",
        "  if flip:\n",
        "    x = torch.flip(x, [3])\n",
        "  return x\n",
        "\n",
        "class CA(torch.nn.Module):\n",
        "  def __init__(self, chn=CHN, hidden_n=128):\n",
        "    super().__init__()\n",
        "    self.chn = chn\n",
        "\n",
        "    # Determine the number of perceived channels\n",
        "    perc_n = perception(torch.zeros([1, chn, 8, 8])).shape[1]\n",
        "\n",
        "    # Approximately equalize the parameter count between model variants\n",
        "    hidden_n = 8 * 1024 // (perc_n + chn)\n",
        "    hidden_n = (hidden_n + 31) // 32 * 32\n",
        "\n",
        "    # Model layers\n",
        "    self.w1 = torch.nn.Conv2d(perc_n, hidden_n, 1)\n",
        "    self.w2 = torch.nn.Conv2d(hidden_n, chn, 1, bias=False)\n",
        "    self.w2.weight.data.zero_()\n",
        "\n",
        "  def forward(self, x, update_rate=UPDATE_RATE):\n",
        "    # Get update and masks\n",
        "    alive = get_alive_mask(x)\n",
        "    y = perception(x)\n",
        "    y = self.w2(torch.relu(self.w1(y)))\n",
        "    b, c, h, w = y.shape\n",
        "    update_mask = (torch.rand(b, 1, h, w) + update_rate).floor()\n",
        "\n",
        "    # Perform update\n",
        "    x = x + y * update_mask\n",
        "    if SCALAR_CHN == CHN:\n",
        "      x = x * alive\n",
        "    else:\n",
        "      state = x[:, :SCALAR_CHN] * alive\n",
        "      angle = x[:, SCALAR_CHN:] % (2. * torch.pi)\n",
        "      x = torch.cat([state, angle], 1)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF4nrfDn5lf4"
      },
      "source": [
        "# Target Legend\n",
        "\n",
        "Select the target of interest from the following\n",
        "\n",
        "* LIZARD:&emsp;ü¶é\n",
        "* HEART:&emsp;‚ù§Ô∏è\n",
        "* SMILEY:&emsp;üòÅ\n",
        "\n",
        "Select the number of seeds and seed radius below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "LPUMYyptdiDx"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W:  64\n"
          ]
        }
      ],
      "source": [
        "#@title Seed and Target Setup {vertical-output:true}\n",
        "\n",
        "SEED_N = 2  #@param{type:\"integer\"}\n",
        "SEED_R = 4  #@param{type:\"integer\"}\n",
        "TARGET = 'COWBOY'  #@param['LIZARD', 'HEART', 'SMILEY']\n",
        "PADDING = 12\n",
        "\n",
        "# emoji = {\n",
        "#     'LIZARD':  'ü¶é',\n",
        "#     'HEART':  '‚ù§Ô∏è',\n",
        "#     'SMILEY': 'üòÅ'\n",
        "# }[TARGET][0]\n",
        "# code = hex(ord(emoji))[2:].lower()\n",
        "url = '../_images/cowboy.png' #https://github.com/googlefonts/noto-emoji/blob/main/png/128/emoji_u%s.png?raw=true'%code\n",
        "target = imread(url, 40)\n",
        "\n",
        "# Show lineup\n",
        "n_seed = seed(1, 40, SEED_N, SEED_R)[0, :4].permute([1, 2, 0]).cpu()\n",
        "imgs = np.stack([n_seed, target], 0)\n",
        "imshow(tile2d(imgs, 2))\n",
        "\n",
        "# Format target\n",
        "target[:, :, :3] *= target[:, :, 3:]\n",
        "target = F.pad(torch.tensor(target).permute(2, 0, 1), [*(PADDING,)*4, 0, 0])\n",
        "W = target.shape[1]\n",
        "\n",
        "print ('W: ', W)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fR5RlK-97Kg2"
      },
      "source": [
        "# Loss Legend\n",
        "\n",
        "Select the loss function of interest from the following\n",
        "\n",
        "* FIXED:&emsp;L2-norm loss\n",
        "* INVARIANT:&emsp;Rotation-invariant loss\n",
        "\n",
        "Select the lower and upper learning rate limits below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "QaqCjNgCmmjS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model name:  steerable_cowboy_fixed_2pt_4r\n"
          ]
        }
      ],
      "source": [
        "#@title Training Utilities and Setup\n",
        "\n",
        "LOSS_FN = 'FIXED'  #@param['FIXED', 'INVARIANT']\n",
        "LOWER_LR = 1e-5  #@param{type:\"number\"}\n",
        "UPPER_LR = 1e-3  #@param{type:\"number\"}\n",
        "MODEL_NAME = f'{MODEL_TYPE}_{TARGET}_{LOSS_FN}_{SEED_N}pt_{SEED_R}r'.lower()\n",
        "MODEL_DIR = os.path.join(MODEL_PATH, MODEL_NAME)\n",
        "if not os.path.exists(MODEL_DIR):\n",
        "    os.mkdir(MODEL_DIR)\n",
        "\n",
        "# if USE_DRIVE and not os.path.exists(MODEL_DIR):\n",
        "#   os.mkdir(MODEL_DIR)\n",
        "\n",
        "def fixed_loss_fn(x, scale=1e3, ax=[]):\n",
        "  return scale * torch.mean(torch.square(x[:, :4] - target[:4]), ax)\n",
        "\n",
        "def unsharp(img):\n",
        "  blured = gaussian_blur(img, (5, 5), (1, 1))\n",
        "  return img + (img - blured) * 2.\n",
        "\n",
        "s = np.sqrt(3) / 2.\n",
        "hex2xy = np.float32([[1., 0.], [.5, s]])\n",
        "xy2hex = torch.tensor(np.linalg.inv(hex2xy))\n",
        "r = torch.linspace(.5/W, 1, W//2)[:, None]\n",
        "a = torch.range(0, W*np.pi) / (W / 2)\n",
        "polar_xy = torch.stack([r*a.cos(), r*a.sin()], -1)[None, :]\n",
        "polar_target = F.grid_sample(unsharp(target[None, :]), polar_xy)\n",
        "\n",
        "x = torch.linspace(-1, 1, W)\n",
        "y, x = torch.meshgrid(x, x)\n",
        "xy_grid = torch.stack([x, y], -1)\n",
        "fft_target = torch.fft.rfft(polar_target).conj()\n",
        "polar_target_sqnorm = polar_target.square().sum(-1, keepdim=True)\n",
        "\n",
        "def invariant_losses_fn(img):\n",
        "  img = unsharp(img)\n",
        "  polar_img = F.grid_sample(img, polar_xy.repeat(len(img), 1, 1, 1), mode='bicubic')\n",
        "  x = torch.fft.rfft(polar_img)\n",
        "  xy = torch.fft.irfft(x*fft_target)\n",
        "  xx = polar_img.square().sum(-1, keepdim=True)\n",
        "  yy = polar_target_sqnorm\n",
        "  sqdiff = xx + yy - 2. * xy\n",
        "  return sqdiff.mean([1, 2])\n",
        "\n",
        "def invariant_loss_fn(img):\n",
        "  return invariant_losses_fn(img).min(-1)[0].mean()\n",
        "\n",
        "loss_fn = {\n",
        "    'FIXED': fixed_loss_fn,\n",
        "    'INVARIANT': invariant_loss_fn,\n",
        "}[LOSS_FN]\n",
        "\n",
        "def circle_masks(n, sz):\n",
        "  x = torch.linspace(-1.0, 1.0, sz)[None, None, :]\n",
        "  y = torch.linspace(-1.0, 1.0, sz)[None, :, None]\n",
        "  center = -torch.rand([2, n, 1, 1]) + 0.5\n",
        "  r = -0.3*torch.rand([n, 1, 1]) + 0.4\n",
        "  x, y = (x-center[0])/r, (y-center[1])/r\n",
        "  mask = (x*x+y*y < 1.0).float()\n",
        "  return mask\n",
        "\n",
        "model = CA()\n",
        "loss_log = []\n",
        "progress = 0\n",
        "with torch.no_grad():\n",
        "  pool = seed(256, W, SEED_N, SEED_R)\n",
        "opt = torch.optim.Adam(model.parameters(), UPPER_LR)\n",
        "lr_sched = torch.optim.lr_scheduler.CyclicLR(opt, LOWER_LR, UPPER_LR, step_size_up=2000, mode='triangular2', cycle_momentum=False)\n",
        "\n",
        "print ('model name: ', MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "OFCqvxs1dr10"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/15001 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\Marco\\Documents\\GitHub\\neural-cellular-automata\\2_blogpost_iso_nca\\blogpost_code.ipynb Cell 9\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Marco/Documents/GitHub/neural-cellular-automata/2_blogpost_iso_nca/blogpost_code.ipynb#X11sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m     imgs \u001b[39m=\u001b[39m to_rgb(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Marco/Documents/GitHub/neural-cellular-automata/2_blogpost_iso_nca/blogpost_code.ipynb#X11sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     imgs \u001b[39m=\u001b[39m imgs\u001b[39m.\u001b[39mpermute([\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m])\u001b[39m.\u001b[39mcpu()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Marco/Documents/GitHub/neural-cellular-automata/2_blogpost_iso_nca/blogpost_code.ipynb#X11sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     imshow(tile2d(imgs, \u001b[39m4\u001b[39;49m), scale\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Marco/Documents/GitHub/neural-cellular-automata/2_blogpost_iso_nca/blogpost_code.ipynb#X11sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m INFO_RATE \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Marco/Documents/GitHub/neural-cellular-automata/2_blogpost_iso_nca/blogpost_code.ipynb#X11sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39mstep_n:\u001b[39m\u001b[39m'\u001b[39m, i, \u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mloss:\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m.\u001b[39mitem(), \u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mlr:\u001b[39m\u001b[39m'\u001b[39m, lr_sched\u001b[39m.\u001b[39mget_lr()[\u001b[39m0\u001b[39m], end\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[1;32mc:\\Users\\Marco\\Documents\\GitHub\\neural-cellular-automata\\2_blogpost_iso_nca\\blogpost_code.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Marco/Documents/GitHub/neural-cellular-automata/2_blogpost_iso_nca/blogpost_code.ipynb#X11sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtile2d\u001b[39m(a, w\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Marco/Documents/GitHub/neural-cellular-automata/2_blogpost_iso_nca/blogpost_code.ipynb#X11sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m   a \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(a)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Marco/Documents/GitHub/neural-cellular-automata/2_blogpost_iso_nca/blogpost_code.ipynb#X11sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m   \u001b[39mif\u001b[39;00m w \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Marco/Documents/GitHub/neural-cellular-automata/2_blogpost_iso_nca/blogpost_code.ipynb#X11sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m     w \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(np\u001b[39m.\u001b[39mceil(np\u001b[39m.\u001b[39msqrt(a\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])))\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\_tensor.py:970\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    968\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[39m.\u001b[39m__array__, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m    969\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 970\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnumpy()\n\u001b[0;32m    971\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    972\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
          ]
        }
      ],
      "source": [
        "#@title Training Loop {vertical-output:true}\n",
        "\n",
        "TRAIN_MODEL = True\n",
        "EPOCHS = 15000  #@param{type:\"number\"}\n",
        "DAMAGE_RATE = 3\n",
        "IMAGE_RATE = 20\n",
        "INFO_RATE = 20\n",
        "SAVE_RATE = 500\n",
        "\n",
        "for _ in tqdm(range(EPOCHS+1)):\n",
        "    if not TRAIN_MODEL:\n",
        "        print ('skipping training')\n",
        "        break\n",
        "    with torch.no_grad():\n",
        "        i = len(loss_log)\n",
        "        batch_idx = np.random.choice(len(pool), 8, replace=False)\n",
        "        x = pool[batch_idx]\n",
        "        # loss_rank = torch.argsort(loss_fn(x, ax=[-2, -3, -1]), descending=True)\n",
        "        # x = x[loss_rank]\n",
        "        \n",
        "        seed_rate = 1  if i < 4000 else 5\n",
        "        if i % seed_rate == 0:\n",
        "            x[:1] = seed(1, W, SEED_N, SEED_R)\n",
        "\n",
        "        if i % DAMAGE_RATE == 0:\n",
        "            damage_mask = 1. - circle_masks(1, W)[:, None]\n",
        "            x[-1:] *= damage_mask\n",
        "\n",
        "    step_n = np.random.randint(64, 96)\n",
        "    overflow_loss = 0.\n",
        "    diff_loss = 0.\n",
        "    target_loss = 0.\n",
        "    last_x = torch.zeros(x.shape)\n",
        "    for _ in range(step_n):\n",
        "        px = x\n",
        "        x = model(x)\n",
        "        diff_loss += (x - px).abs().mean()\n",
        "        overflow_loss += (x - x.clamp(-2., 2.))[:, :SCALAR_CHN].square().sum()\n",
        "\n",
        "    target_loss += loss_fn(x[:, :target.shape[0]])\n",
        "    diff_loss *= 10.\n",
        "    loss = target_loss + overflow_loss + diff_loss\n",
        "    # if loss.isnan():\n",
        "    #   # TODO: reload model from last checkpoint\n",
        "    #   print('\\nWARNING: NaN')\n",
        "    #   pool[batch_idx] = seed(8, W, SEED_N, SEED_R)\n",
        "    #   continue\n",
        "\n",
        "    with torch.no_grad():\n",
        "        loss.backward()\n",
        "        for p in model.parameters():\n",
        "            p.grad /= (p.grad.norm() + 1e-8)\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "        lr_sched.step()\n",
        "        pool[batch_idx] = x\n",
        "        loss_log.append(loss.item())\n",
        "\n",
        "    if i % IMAGE_RATE == 0:\n",
        "        clear_output(True)\n",
        "        pl.plot(loss_log, '.', alpha=.1)\n",
        "        pl.yscale('log')\n",
        "        pl.ylim(np.min(loss_log), loss_log[0])\n",
        "        pl.show()\n",
        "        imgs = to_rgb(x)\n",
        "        imgs = imgs.permute([0, 2, 3, 1]).cpu()\n",
        "        imshow(tile2d(imgs.detach(), 4), scale=2)\n",
        "\n",
        "    if i % INFO_RATE == 0:\n",
        "        print('\\rstep_n:', i, '\\tloss:', loss.item(), '\\tlr:', lr_sched.get_lr()[0], end='')\n",
        "\n",
        "    if i % SAVE_RATE == 0:\n",
        "        progress = i\n",
        "        torch.save(model, os.path.join(MODEL_DIR, f'{MODEL_NAME}_{i}.pt'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8Hf6DHbym4oy"
      },
      "outputs": [],
      "source": [
        "#@title Model Demo {vertical-output:true}\n",
        "\n",
        "FRAMES = 500  #@param{type:\"integer\"}\n",
        "progress = 15000\n",
        "\n",
        "model_name = f'{MODEL_TYPE}_{TARGET}_{LOSS_FN}_{SEED_N}pt_{SEED_R}r_{progress}'.lower()\n",
        "model_path = os.path.join(MODEL_DIR, f'{model_name}.pt')\n",
        "demo_path = os.path.join(MODEL_DIR, f'{model_name}.mp4')\n",
        "\n",
        "try:\n",
        "  model = torch.load(model_path)\n",
        "  vidgen(demo_path, model, n_frames=FRAMES, p=SEED_N, r=SEED_R)\n",
        "except FileNotFoundError:\n",
        "  print(f'Model \"{model_name}\" not found.')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
